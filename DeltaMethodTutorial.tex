% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Statistical Inference for Functionals in Epidemiology using the Delta Method and Influence Functions: An Applied and Reproducible Tutorial},
  pdfauthor={Miguel Angel Luque Fernandez, MA, MPH, MSc, Ph.D},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

\title{Statistical Inference for Functionals in Epidemiology using the Delta
Method and Influence Functions: An Applied and Reproducible Tutorial}
\author{Miguel Angel Luque Fernandez, MA, MPH, MSc, Ph.D}
\date{02/03/2020 \url{https://scholar.harvard.edu/malf/home}}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
Tweet

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

A fundamental problem in inferential statistics is to approximate the
distribution of a statistic calculated from a probability sample of
data. The statistic is usally a parameter estimate that characterizes
the sampling variability of the estimate (Boos and Stefanski, 2013).
Becasue the number and type of inference problems for which exact
parametric distributions can be determined is limited the approximation
of the distribution of the statistic is necesary. Approximate inference
via determination of the asymptotic distribution of the statistics is a
techique that epidemiologists and applied statisticians, we have been
using for years. We name it the ``Delta Method''. The delta method is a
theorem which states that a smooth function of an asymptotically normal
estimator is also asymptotically normally distributed. It can also be
viewed as a technique for approximating variance of a functional i.e., a
nonlinear function of a random variable that can be approximate by
averages (Armitage and Colton, 2005; Boos and Stefanski, 2013).

For instance, in epidemiology we use routinely the delta method to
compute the standard error (SE) of functionals as the risk difference
(RD), the risk ratio (RR), and the odds ratio (OR) (Agresti, 2010).
Often in addition to reporting parameters fit by a model, we need to
report some marginal transformation of these parameters. The
transformation can generate the point estimates of our desired values,
but the SE of these point estimates are not so easily calculated and we
need to approximate the distribution of the SE for statistical inference
using the delta method.

More recently, in causal inference epidemiologists compute the average
treatment effect (ATE) using G-computation, a generalization of the
standardization, among other estimations techinques as the inverse
probability of treatment weights (Rubin, 2007). The ATE is defined by an
average which is a function of two random variables (i.e., the potential
outcomes E(Y1) and E(Y0)) (Rubin, 2007; Gutman and Rubin, 2015). These
two random variables are predicted from the coefficients of the
parameters fitted in two different regression models (Greenland and
Robins, 1986). The functional delta method applied to an estimator
implicitly defined by averages, also known as M-estimator, is the method
used to approximate the variance of the estimator (i.e., G-computation
for the ATE) and can be used to estimate the standar error (SE) of the
ATE under some theoretical assumptioms from Semiparametric and Empirical
Process theory (Boos and Stefanski, 2013; Kennedy, 2016; Kennedy
\emph{et al.}, 2017). We assume that the estimator used to derive the
ATE is pathwise differentiable and an asymptotically regular annd linear
estimator (Boos and Stefanski, 2013; Kennedy, 2016; Kennedy \emph{et
al.}, 2017). We, applied statisticians and epidemiologists should knonw
how to determine when a large sample approximation to the distribution
of a statistic is appropiate, how to derive the approximation, and how
to use it for inference applications. In this tutorial we introduce the
use of the functional delta method in Epidemiology from a practical
perspective including boxes with code in R and Stata software to allow
readers learning by doing.

\hypertarget{the-delta-method}{%
\section{The Delta Method}\label{the-delta-method}}

Note that although the delta method is often appropriate to use with
large samples, other methods can be used to estimate standard errors,
such as the bootstrap (Efron and Tibshirani, 1993; Efron and Efron,
1982). Essentially, the delta method involves calculating the expansion
up to the first order of a Taylor series approximation of a function
(Herberg and Bristol, 1962) to derive the empirical value and the SE of
an estimator. When a statistic can be approximate by an average, the
approximating average is usually an average of some function of the
sample values and can be writen in the form:

\[f(\hat\theta)-f(\theta)\;\approx\;f'(\theta)(\hat\theta\,-\,\theta)\,+\,\text{Op}(\frac{1}{\sqrt(n)}) \;\;(1).\]
Where \(f(\hat\theta)\) that can be interpreted as the large sample
stochastic limit of \(f(\theta)\) and
\(f'(\theta)(\hat\theta\,-\,\theta)\) is a function often called the
Influence Curve and the remaider \(\text{Op}(\frac{1}{\sqrt(n)})\) is
negligibly small as the sample size increases (Boos and Stefanski, 2013)
i.e., asymptotically converges to zero,

\[\text{Op}(\frac{1}{\sqrt(n)}) \rightarrow 0 \;,\] and the distribution
of the statistic converges in probability to a normally distrubuted
random variable with mean zero and finite variance.

\[\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,,var(\theta)).\]
Therefore, suppose that \((\hat\theta)\) is a multivariable sample mean
and \(f\) is a smooth scalar valued function. Then, the mean of an
estimator \(f(\hat\theta)\) can be approximate with the population mean
\(f(\theta)\) plus the influence curve derived as the first derivative
of the estimator times the difference between the sample population mean
minus the mean of the estimator approximated as follows:

\[f(\hat\theta)\approx\;f(\theta)\,+\,f'(\theta)(\hat\theta\,-\,\theta)\, \;\;(2)\]
\[f(\bar{Y})\approx\;f(\mu)\,+\,f'(\mu)(\bar{Y},-\,\mu)\, \;= n^{-1}\sum{f(\mu)\,+\,f'(\mu)(\bar{Y_{i}}\,-\,\mu)},\]
Note that the variance (i.e., standard error of the estimator) is then
estimated using the IC (i.e., \(f'(\theta)(\hat\theta\,-\,\theta)\)) and
\(f'(\mu)(\bar{Y_{i}}\,-\,\mu)\) for the multivariate sample mean. Now
lets suppose that we want to consider a pair of independent identically
distributed (iid) multivariable random sample means (i.e., Y1 and Y2)
and we want to derive the SE for the ratio estimator
\(f(\bar{Y})\,=\,\bar{Y_{1}}/\bar{Y_{2}}\) where \(\mu_{2}\neq\,0\). In
this case the IC for the ratio will be estimated as follows:

\[f'(\mu)\,=\, \frac{1}{\mu_{1}}\,-\,\frac{\mu_{1}}{\mu_{x}^{2}}\] Then
\[IC = \frac{1}{\mu_{1}}{}(\bar{Y_{i1}}\,-\,\mu_{1})\,-\,\frac{\mu_{1}}{\mu_{x}^{2}}(\bar{Y_{i2}}\,-\,\mu_{2})\]

For more than two iid multivariable random variables we first get the
Taylor series approximation of the function using the first two terms of
the expansion of the transformation function about the mean of of the
random variable but we need to compute a vector of partial derivatives
of \(\theta\) (i.e., \(\nabla \theta\) which is the gradient of
\(\theta\)). We can then take the variance of this approximation to
estimate the variance of \(\theta\) and thus the SE of a transformed
parameter. The first two terms of the Taylor expansion are then an
approximation for \(\hat\theta\) as follows:,

\[f(\hat\theta) \approx\, f(\theta) \,+\,\nabla(\theta)^T\,(\hat\theta\,-\,\theta)\; (3).\]
Where \(\nabla \theta\) is the gradient of \(\theta\), or a vector of
partial derivatives of \(\theta\). We can then take the variance of this
approximation to estimate the variance of \(\theta\) and thus the SE of
a transformed parameter.

In summary, for observed data \(\text{O}_i\), i = 1, \ldots, n an
asymptotically linear estimator \(\hat\Psi\) of an estimand \(\Psi\), is
an estimator that can be represented as follows:
\[\sqrt(n)(\hat\Psi\,-\,\Psi);=\;\frac{1}{n}\sum_{i=1}^n \text{D}(\text{O}_{i})\,+\,\text{Op}(\frac{1}{\sqrt(n)})\;\;(4).\]
Where \(\text{D}(\text{O}_{i})\) represents the IC of the estimator:

\[\text{D}(\text{O}_{i})=f'(\theta)(\hat\theta\,-\,\theta).\] In other
words, the difference between the estimator and estimand can be
represented as the sample mean of a fixed function (the \emph{``IC''})
plus a remainder term that must converge to 0 at a rate faster than
\(\frac{1}{\sqrt{n}}\). The estimated IC provides an asymptotic variance
estimate for the estimator (i.e., we can apply the \emph{Central Limit
Theorem} and compute \emph{Wald} type confidence intervals) (Laan and
Rose, 2011).

\hypertarget{delta-method-for-the-a-simple-sample-mean}{%
\section{Delta method for the a simple sample
mean}\label{delta-method-for-the-a-simple-sample-mean}}

\begin{verbatim}
## [1] 0.508518
\end{verbatim}

\includegraphics{DeltaMethodTutorial_files/figure-latex/unnamed-chunk-1-1.pdf}

\begin{verbatim}
## [1] 0.508518
\end{verbatim}

\begin{verbatim}
## [1] 0.4905607 0.5264753
\end{verbatim}

\includegraphics{DeltaMethodTutorial_files/figure-latex/unnamed-chunk-1-2.pdf}

\begin{verbatim}
## [1] 0.009161893
\end{verbatim}

\begin{verbatim}
## [1] 0.009161893
\end{verbatim}

\begin{verbatim}
## [1] 0.508518
\end{verbatim}

\begin{verbatim}
## [1] 0.4905607 0.5264753
\end{verbatim}

\begin{verbatim}
## Loading required package: car
\end{verbatim}

\begin{verbatim}
## Loading required package: carData
\end{verbatim}

\begin{verbatim}
## Loading required package: sandwich
\end{verbatim}

\begin{verbatim}
##    parameter name
##  (Intercept)   b0
## 
##     Estimate        SE     2.5 % 97.5 %
## b0 0.5085180 0.0091619 0.4905610 0.5265
\end{verbatim}

\hypertarget{delta-method-for-the-most-commun-epidemiological-estimands-risk-differeces-relative-risk-and-odds-ratio}{%
\section{Delta method for the most commun epidemiological estimands:
Risk differeces, relative risk, and odds
ratio}\label{delta-method-for-the-most-commun-epidemiological-estimands-risk-differeces-relative-risk-and-odds-ratio}}

Using the classical 2 by 2 epidemiological table presenting outcome
counts by the levels of a risk factor, we are going to derive the SE for
the for the RD, RR and OR using the Delta Method

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.24\columnwidth}\raggedright
Risk\strut
\end{minipage} & \begin{minipage}[b]{0.33\columnwidth}\raggedright
Alive\strut
\end{minipage} & \begin{minipage}[b]{0.33\columnwidth}\raggedright
Dead\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.24\columnwidth}\raggedright
Exposed\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\(\text{n}_{11}\) = (\(\text{p}_{1}\))\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\(\text{n}_{21}\) = (\(\text{p}_{2}\))\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
Unexposed\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\(\text{n}_{12}\) = (1 -\(\text{p}_{1}\))\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\(\text{n}_{22}\) = (1-\(\text{p}_{2}\))\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\raggedright
N\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\(\text{N}_{1}\)\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\(\text{N}_{2}\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{risk-difference-rd}{%
\subsubsection{Risk Difference (RD)}\label{risk-difference-rd}}

The risk difference is defined as follows (Rothman \emph{et al.}, 2008):

\[\widehat{RD} \,=\, \hat \theta_{1}\,-\,\hat \theta_{2} \,=\, \hat p_{1}\,-\,\hat p_{2}.\]
Assuming that the probability of the event (\textbf{p}) can be modelled
using a Bernoulli distribution with range \(0\;\leq \text{p} \leq\;1\),
expectation of p is E(p) = p, and the variance var(p) = p(1-p)/n.~

Given that
\[\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,, var(\theta)).\]
and using for the probability (p) the formula in (4)

\[SE(\widehat{RD})\,=f'(\theta)(var(\theta))\;=\; \sqrt{\frac{p(1-p)}{n}}\]
We have that for the RD the SE is\\
\[SE(\widehat{RD})\,=\, \sqrt{\frac{(1-\hat p_{1})}{n_{1}}}\,+\,\sqrt{\frac{(1-\hat p_{2})}{n_{2}}}\,=\,\sqrt{\frac{(1-\hat p_{1})}{n_{1}}\,+\,\frac{(1-\hat p_{2})}{n_{2}}}\]

\hypertarget{risk-ratio-rr}{%
\subsubsection{Risk Ratio (RR)}\label{risk-ratio-rr}}

\[\widehat{RR} \,=\, \frac{\hat \theta_{1}}{\hat \theta_{2}} \,=\, log\left(\frac{\hat p_{1}}{\hat p_{2}}\right)\,=\,log(\hat p_{1}) + log(\hat p_{2})\]
Given that
\[\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,, var(\theta))\]
then
\[SE(log(\hat{\theta}))\,=f'(\theta)(var(\theta))\;=\;\frac{1}{p}\sqrt{\frac{p(1-p)}{n}}\,=\,\sqrt{\frac{(1-p)}{pn}}\]
We have that
\[SE(log(\widehat{RR}))\,=\, \sqrt{\frac{(1-\hat p_{1})}{\hat p_{1}n_{1}}}\,+\,\sqrt{\frac{(1-\hat p_{2})}{\hat p_{2}n_{2}}}\,=\,\sqrt{\frac{(1-\hat p_{1})}{\hat p_{1}n_{1}}\,+\,\frac{(1-\hat p_{2})}{\hat p_{2}n_{2}}}\]

\hypertarget{odds-ratio-or}{%
\subsubsection{Odds Ratio (OR)}\label{odds-ratio-or}}

\[\widehat{OR} \,=\, \frac{\hat \theta_{1}}{\hat \theta_{2}} \,=\, log\left(\frac{\hat p_{1}/(1-\hat p_{1})}{\hat p_{2}/(1-\hat p_{2})}\right)\,=\,log\left(\frac{\text{n}_{11}\text{n}_{22}}{\text{n}_{12}\text{n}_{21}}\right)\,\]
Given that
\[\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,, var(\theta))\]
then
\[SE(log(\hat{\theta}))\,=f'(\theta)(var(\theta))\;=\;\frac{1}{p(1-p)}\sqrt{\frac{p(1-p)}{n}}\,=\,\sqrt{\frac{1}{n}}\]

We have that
\[SE(log(\widehat{OR}))\,=\, \sqrt{\frac{1}{n_{11}}\,+\,\frac{1}{n_{12}}\,+\,\frac{1}{n_{21}}\,+\,\frac{1}{n_{22}}}\]
Finally, using the \emph{Central Limit Theorem} the Wald type 95\%
Confidence Intervals for the RD, RR and OR can be estimated as follows:

\[95\%\text{CI}\,=\,1.96\times\text(SE(\hat\theta))\]

\hypertarget{empirical-example}{%
\section{Empirical example}\label{empirical-example}}

To illustrate the use of the Delta Method we are going to generate data
based on a cancer epidemiology example where we want to estimate the
effect of comorbidities (binary indicator) on one-year cancer mortality
controlling for the confounding effect of age in a cohort of 1,000
patients in their middle age. We assume that it is an extremely lethal
type of cancer (i.e., pancreatic cancer) thus we can expect high
one-year mortality rate among younger patients. Age in years was
generated as a normal random variable with mean 65 years and standard
deviation 5 years. Comorbidities was generated as a binary indicator and
as a function of age using a binomial model. Patients\(’\) one-year
mortality rate was generated as a function of the patients\(’\) age and
the presence of comorbidities using a binomial distribution. The data
generation and models specifications are provide here below:

\begin{verbatim}
## -- Attaching packages ----------------------------------------- tidyverse 1.3.0 --
\end{verbatim}

\begin{verbatim}
## v ggplot2 3.2.1     v purrr   0.3.3
## v tibble  2.1.3     v dplyr   0.8.4
## v tidyr   1.0.2     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.4.0
\end{verbatim}

\begin{verbatim}
## -- Conflicts -------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
## x dplyr::recode() masks car::recode()
## x purrr::some()   masks car::some()
\end{verbatim}

Here we describe the data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Describing the data}
\NormalTok{data <-}\StringTok{ }\KeywordTok{generateData}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{seed =} \DecValTok{777}\NormalTok{) }
\KeywordTok{str}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    1000 obs. of  3 variables:
##  $ Y   : int  0 1 1 0 0 1 0 0 1 1 ...
##  $ cmbd: int  0 0 0 0 0 0 0 0 0 1 ...
##  $ age : num  67.4 63 67.6 63 73.2 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarize}\NormalTok{(}
\NormalTok{  data,}
  \DataTypeTok{Status =} \KeywordTok{mean}\NormalTok{(Y), }
  \DataTypeTok{Comorbidities =} \KeywordTok{mean}\NormalTok{(cmbd),}
  \DataTypeTok{Age =}  \KeywordTok{mean}\NormalTok{(age)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Status Comorbidities      Age
## 1  0.411         0.089 65.00988
\end{verbatim}

\hypertarget{delta-method-for-a-singly-univariate-parameter}{%
\subsection{Delta Method for a singly univariate
parameter}\label{delta-method-for-a-singly-univariate-parameter}}

First, we are going to derive the SE for the single probability or risk
of death (the univariate case). We compute the risk of death in our
sample and its variance as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Risk of death}
\NormalTok{p_death =}\StringTok{ }\KeywordTok{mean}\NormalTok{(data}\OperatorTok{$}\NormalTok{Y)}
\KeywordTok{print}\NormalTok{(p_death)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.411
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Varianze for the risk of death}
\NormalTok{ n =}\StringTok{ }\KeywordTok{nrow}\NormalTok{(data)}
\NormalTok{ var_p_death =}\StringTok{ }\NormalTok{p_death }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{p_death) }\OperatorTok{/}\StringTok{ }\NormalTok{n}
 \KeywordTok{print}\NormalTok{(var_p_death)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.000242079
\end{verbatim}

Now, let be f(x) = p.~Then the first derivative of f′(x) = 1. So the
variance of the risk of death can be estimated using (4) as:

\[\text{Var(P(death))}\,=\,1\times\left[\frac{\text{p(1 - p)}}{n}\right].\]

This can be implemented in the following R code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dev_p_death =}\StringTok{ }\DecValTok{1}
\NormalTok{se_risk =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((dev_p_death) }\OperatorTok{*}\StringTok{ }\NormalTok{var_p_death)}
\KeywordTok{print}\NormalTok{(se_risk)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01555889
\end{verbatim}

To check that our results are consistent with the implementation of the
Delta Method function provide by the \textbf{msm} R package used for
advanced Geographical Analysis (Kavroudakis and others, 2015).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("msm")}
\KeywordTok{library}\NormalTok{(msm)}
\NormalTok{se_risk_delta =}\StringTok{ }\KeywordTok{deltamethod}\NormalTok{(}\DataTypeTok{g =} \OperatorTok{~}\StringTok{ }\NormalTok{x1, }
                            \DataTypeTok{mean =}\NormalTok{ p_death,         }
                            \DataTypeTok{cov  =}\NormalTok{ var_p_death)}
\KeywordTok{print}\NormalTok{(se_risk_delta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01555889
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"Are the same se_risk and se_risk_delta?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Are the same se_risk and se_risk_delta?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  se_risk }\OperatorTok{==}\StringTok{ }\NormalTok{se_risk_delta,}
  \KeywordTok{print}\NormalTok{(}\StringTok{"Yes"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Yes"
\end{verbatim}

\begin{verbatim}
## [1] "Yes"
\end{verbatim}

\hypertarget{conditional-odds-ratio-cor-multivariable-case}{%
\subsection{Conditional Odds Ratio (COR): Multivariable
Case}\label{conditional-odds-ratio-cor-multivariable-case}}

Let\('\)s now compute the SE for the COR derived from a multivariable
logistic regression model. Note that the COR transformation is a
function of the regression coefficients from the logistic model. First
we estimate the conditional probability of the risk of death for those
patients with comorbidities adjusting for age. The model summary is
described here below. The probability of death for a cancer patient in
our sample with comorbidities compared with a patient without
comorbidities and in average with the same age is approximately 40\%
higher:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{cmbd, }\DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{family =}\NormalTok{ binomial)}
\KeywordTok{summary}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Y ~ age + cmbd, family = binomial, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.2756  -1.0264  -0.9628   1.3141   1.5119  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)  
## (Intercept)  1.36593    0.85231   1.603   0.1090  
## age         -0.02703    0.01307  -2.069   0.0386 *
## cmbd         0.33029    0.22385   1.475   0.1401  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1354.4  on 999  degrees of freedom
## Residual deviance: 1347.4  on 997  degrees of freedom
## AIC: 1353.4
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b1 <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(m1)[}\DecValTok{3}\NormalTok{]}
\KeywordTok{cat}\NormalTok{(}\StringTok{"One-year mortality risk for patients with comorbidities vs no comorbidities is:"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## One-year mortality risk for patients with comorbidities vs no comorbidities is:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(b1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     cmbd 
## 1.391365
\end{verbatim}

We now, can derive the SE for the conditional OR using the formula (4)
for the multivariate case. Note that the first derivative for the
exponential function is equal to exponential and that to get the
covariance of the parameter fitted in the model we use the command
``vcov'' in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grad <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(b1)}
\KeywordTok{vcov}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             (Intercept)           age         cmbd
## (Intercept)  0.72643498 -0.0111017571 -0.020775569
## age         -0.01110176  0.0001707399  0.000249041
## cmbd        -0.02077557  0.0002490410  0.050107817
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vb1 <-}\StringTok{ }\KeywordTok{vcov}\NormalTok{(m1)[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{]}
\NormalTok{se <-}\StringTok{ }\NormalTok{grad }\OperatorTok{%*%}\StringTok{ }\NormalTok{vb1 }\OperatorTok{%*%}\StringTok{ }\NormalTok{grad}
\NormalTok{se_cor <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(se); se_cor}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 0.311454
\end{verbatim}

Now, we have to check that our results are consistent with the
implementation of the Delta Method function provide by the \textbf{msm}
R package (Kavroudakis and others, 2015)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se_cor_delta <-}\StringTok{ }\KeywordTok{deltamethod}\NormalTok{(}\OperatorTok{~}\StringTok{ }\KeywordTok{exp}\NormalTok{(x1), b1, vb1); se_cor_delta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.311454
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"Are the same se_cor and se_cor_delta?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Are the same se_cor and se_cor_delta?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  se_cor }\OperatorTok{==}\StringTok{ }\NormalTok{se_cor_delta,}
  \KeywordTok{print}\NormalTok{(}\StringTok{"Yes"}\NormalTok{),}\KeywordTok{print}\NormalTok{(}\StringTok{"No"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Yes"
\end{verbatim}

\begin{verbatim}
##      [,1] 
## [1,] "Yes"
\end{verbatim}

\hypertarget{conditional-risk-ratio-crr-multivariable-case}{%
\subsection{Conditional Risk Ratio (CRR): multivariable
case}\label{conditional-risk-ratio-crr-multivariable-case}}

Let\('\)s now compute the SE for the marginal multivariable RR derived
from the predicted probabilities of a multivariable logistic regression
model. First we fit the model with the binary indicator of one-year
mortality as dependent variable and patients\(’\) age and comorbidities
as independent variables. Then, from the fitted model and using the
\textbf{predict} function we derive the probability of death from
pancreatic cancer among patients aged 45 years and with comorbidities
versus patients aged 75 years old with no comorbidities. Finally, we
compute the conditional RR as the ratio between both probabilities. As
we can see the risk of death in a population where cancer patients were
aged 45 years with comorbidities is approximately 81\% higher than the
risk of one-year mortality among cancer patients aged 75 years and with
no comorbidities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{cmbd, }\DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{family =}\NormalTok{ binomial)}
\NormalTok{p75 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(m2, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{age =} \DecValTok{75}\NormalTok{, }\DataTypeTok{cmbd =} \DecValTok{0}\NormalTok{), }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{p45 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(m2, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{age =} \DecValTok{45}\NormalTok{, }\DataTypeTok{cmbd =} \DecValTok{1}\NormalTok{), }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{mrr <-}\StringTok{ }\NormalTok{p45 }\OperatorTok{/}\StringTok{ }\NormalTok{p75;}
\KeywordTok{cat}\NormalTok{(}\StringTok{"Conditional Risk Ratio: "}\NormalTok{, mrr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Conditional Risk Ratio:  1.814684
\end{verbatim}

Let\('\)s now to compute the SE for the marginal RR. Note that the
relative risk transformation is a function of the regression
coefficients. First, we should define the conditional probability in
terms of the regression coefficients. In our model, given patients age
and comorbidities, the probability of one-year mortality is:
\[\text{P(Y=1|X)}\,=\,\frac{1}{1\,+\,\text{exp}(−\sum_{i=1}^k \beta(X))}\]
Where k is the number of parameters in the model
\(\beta\,=\,(\beta_{0},\beta_{1},\beta_{2})\). Therefore, the
probabality of one-year mortality for cancer patients aged 45 years with
comorbidities is
\[\text{P(1)(Y = 1|X1 = 45, X3 = 1)}\,=\,\frac{1}{1\,+\,exp(−\beta_{0}\,-\,\beta_{1}\times 45\,-\,\beta_{2}\times 1)},\]
and the probabality of one-year mortality for cancer patients aged 75
years with no comorbidities is
\[\text{P(2)(Y = 1|X2 = 75, X4 = 0)}\,=\,\frac{1}{1\,+\,exp(−\beta_{0}\,-\,\beta_{1}\times 75\,-\,\beta_{2}\times 0)}.\]
Note that the CRR is a function of the regression coefficients from the
logistic model. Thus, now we use the equation in (3) to get:
\[\text{f(x)}\,=\,\frac{\frac{1}{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x1\,-\,\beta_{2} x3)}}{\frac{1}{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2}x4)}},\]
which simplifies to:
\[\text{f(x)}\,=\,\frac{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4)}{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x1\,-\,\beta_{2}x3)}.\]
We now need to solve the partial derivatives for f(x) but one can use
the online open source available software ``Wolfram alpha:
\url{https://www.wolframalpha.com/}'' to readily get the results for the
partial derivatives of f(x) and then apply the formula (3). Using the
product and chain rules, we obtain the following partial derivatives:
\[\frac{df}{d\beta_{0}}\,=\,\text{−exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times \text{p}\,+\,(1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times \text{p}(1\,–\,\text{p}),\]
then,
\[\frac{df}{d\beta_{1}}\,=\,\text{−exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times x2 \times \text{p}\,+\,(1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times x1 \times \text{p}(1\,–\,\text{p}),\]
and,
\[\frac{df}{d\beta_{2}}\,=\,\text{−exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times x4 \times \text{p}\,+\,(1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x3\,-\,\beta_{2} x4))\times x3 \times \text{p}(1\,–\,\text{p})\]
where p is
\[\text{P(Y=1|X1,X2)}\,=\,\frac{1}{1\,+\text{exp}(−\beta_{0}\,-\,\beta_{1}x1\,-\,\beta_{2}x3)},\]
i.e., the probability of one-year mortality for cancer patients aged 45
years with comorbidities.

Let's calculate our partial derivative in R as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\DecValTok{45}
\NormalTok{x2 <-}\StringTok{ }\DecValTok{75}
\NormalTok{x3 <-}\StringTok{ }\DecValTok{1}
\NormalTok{x4 <-}\StringTok{ }\DecValTok{0}
\NormalTok{b0 <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(m2)[}\DecValTok{1}\NormalTok{]}
\NormalTok{b1 <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(m2)[}\DecValTok{2}\NormalTok{]}
\NormalTok{b2 <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(m2)[}\DecValTok{3}\NormalTok{]}
\NormalTok{e1 <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{b0 }\OperatorTok{-}\StringTok{ }\DecValTok{45}\OperatorTok{*}\NormalTok{b1 }\OperatorTok{-}\StringTok{ }\DecValTok{1}\OperatorTok{*}\NormalTok{b2)}
\NormalTok{e2 <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{b0 }\OperatorTok{-}\StringTok{ }\DecValTok{75}\OperatorTok{*}\NormalTok{b1 }\OperatorTok{-}\StringTok{ }\DecValTok{0}\OperatorTok{*}\NormalTok{b2)}
\NormalTok{p1 <-}\StringTok{ }\DecValTok{1} \OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{e1)}
\NormalTok{p2 <-}\StringTok{ }\DecValTok{1} \OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{e2)}
\NormalTok{dfdb0 <-}\StringTok{ }\OperatorTok{-}\NormalTok{e2}\OperatorTok{*}\NormalTok{p1 }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{e2)}\OperatorTok{*}\NormalTok{p1}\OperatorTok{*}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{p1)}
\NormalTok{dfdb1 <-}\StringTok{ }\OperatorTok{-}\NormalTok{x2}\OperatorTok{*}\NormalTok{e2}\OperatorTok{*}\NormalTok{p1 }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{e2)}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{p1}\OperatorTok{*}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{p1)}
\NormalTok{dfdb2 <-}\StringTok{ }\OperatorTok{-}\NormalTok{x4}\OperatorTok{*}\NormalTok{e2}\OperatorTok{*}\NormalTok{p1 }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{e2)}\OperatorTok{*}\NormalTok{x3}\OperatorTok{*}\NormalTok{p1}\OperatorTok{*}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{p1)}
\NormalTok{grad <-}\StringTok{ }\KeywordTok{c}\NormalTok{(dfdb0, dfdb1, dfdb2)}
\NormalTok{vG <-}\StringTok{ }\KeywordTok{t}\NormalTok{(grad) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{vcov}\NormalTok{(m2) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(grad)}
\NormalTok{se_crr <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(vG));se_crr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3653979
\end{verbatim}

Now, let\('\)s again to check if our results are consistent with the
implementation of the Delta Method function provide by the \textbf{msm}
R package (Kavroudakis and others, 2015). We obtain the same results for
the SE of the crr computed before (0.3653979)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se_crr_delta <-}\StringTok{ }\KeywordTok{deltamethod}\NormalTok{( }\OperatorTok{~}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{x1 }\DecValTok{-75}\OperatorTok{*}\NormalTok{x2 }\DecValTok{-0}\OperatorTok{*}\NormalTok{x3)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{x1 }\DecValTok{-45}\OperatorTok{*}\NormalTok{x2 }\DecValTok{-1}\OperatorTok{*}\NormalTok{x3)), }
             \KeywordTok{c}\NormalTok{(b0, b1, b2), }
             \KeywordTok{vcov}\NormalTok{(m2)}
\NormalTok{             ); se_crr_delta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3653979
\end{verbatim}

Finally, let\('\)s compute the 95\% confidence intervals (CI):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lb <-}\StringTok{ }\NormalTok{mrr }\OperatorTok{-}\StringTok{ }\FloatTok{1.96}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(vG)}
\NormalTok{ub <-}\StringTok{ }\NormalTok{mrr }\OperatorTok{+}\StringTok{ }\FloatTok{1.96}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(vG)}
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ Conditional Risk Rartio (95%CI): "}\NormalTok{) ; }\KeywordTok{cat}\NormalTok{(mrr, }\StringTok{"("}\NormalTok{, lb,}\StringTok{","}\NormalTok{, ub,}\StringTok{")"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Conditional Risk Rartio (95%CI):
\end{verbatim}

\begin{verbatim}
## 1.814684 ( 1.098504 , 2.530864 )
\end{verbatim}

\hypertarget{marginal-causal-risk-ratio-for-the-potential-outcomes-and-based-on-the-aiptw-estimator}{%
\subsection{Marginal Causal Risk Ratio for the potential outcomes and
based on the AIPTW
estimator}\label{marginal-causal-risk-ratio-for-the-potential-outcomes-and-based-on-the-aiptw-estimator}}

Imagine now, that we want to emulate a clinical trial where we would
like to estimate the effect of cancer treatment on the overall average
population marginal one-year risk of death, standardized across all the
levels of patients age, TNM cancer stage, and gender. In R we create a
function to generate the simulated data replicating the DAG from Figure
1:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Y: mortality binary indicator (1 death, 0 alive)
\item
  A: binary treatment (1 Chemotherapy, 0 Radiotherapy )\\
\item
  W1: Gender (1 male; 0 female)\\
\item
  W2: Age at diagnosis (0 \textless65; 1 \textgreater=65)\\
\item
  W3: Cancer TNM classification (scale from 1 to 4; 1: early stage no
  metastasis; 4: advanced stage with metastasis)\\
\item
  W4: Comorbidities (scale from 1 to 5)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\NormalTok{generateData <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n)\{}
\NormalTok{  w1 <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(n, }\DataTypeTok{size=}\DecValTok{1}\NormalTok{, }\DataTypeTok{prob=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{  w2 <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(n, }\DataTypeTok{size=}\DecValTok{1}\NormalTok{, }\DataTypeTok{prob=}\FloatTok{0.65}\NormalTok{)}
\NormalTok{  w3 <-}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{runif}\NormalTok{(n, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\DecValTok{4}\NormalTok{), }\DataTypeTok{digits=}\DecValTok{3}\NormalTok{)}
\NormalTok{  w4 <-}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{runif}\NormalTok{(n, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\DecValTok{5}\NormalTok{), }\DataTypeTok{digits=}\DecValTok{3}\NormalTok{)}
\NormalTok{  A  <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(n, }\DataTypeTok{size=}\DecValTok{1}\NormalTok{, }\DataTypeTok{prob=} \KeywordTok{plogis}\NormalTok{(}\OperatorTok{-}\FloatTok{0.4} \OperatorTok{+}\StringTok{ }\FloatTok{0.2}\OperatorTok{*}\NormalTok{w2 }\OperatorTok{+}\StringTok{ }\FloatTok{0.15}\OperatorTok{*}\NormalTok{w3 }\OperatorTok{+}\StringTok{ }\FloatTok{0.2}\OperatorTok{*}\NormalTok{w4 }\OperatorTok{+}\StringTok{ }\FloatTok{0.15}\OperatorTok{*}\NormalTok{w2}\OperatorTok{*}\NormalTok{w4))}
  \CommentTok{# counterfactual}
\NormalTok{  Y}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(n, }\DataTypeTok{size=}\DecValTok{1}\NormalTok{, }\DataTypeTok{prob=} \KeywordTok{plogis}\NormalTok{(}\OperatorTok{-}\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{1} \FloatTok{-0.1}\OperatorTok{*}\NormalTok{w1 }\OperatorTok{+}\StringTok{ }\FloatTok{0.3}\OperatorTok{*}\NormalTok{w2 }\OperatorTok{+}\StringTok{ }\FloatTok{0.25}\OperatorTok{*}\NormalTok{w3 }\OperatorTok{+}\StringTok{ }\FloatTok{0.2}\OperatorTok{*}\NormalTok{w4 }\OperatorTok{+}\StringTok{ }\FloatTok{0.15}\OperatorTok{*}\NormalTok{w2}\OperatorTok{*}\NormalTok{w4))}
\NormalTok{  Y}\FloatTok{.0}\NormalTok{ <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(n, }\DataTypeTok{size=}\DecValTok{1}\NormalTok{, }\DataTypeTok{prob=} \KeywordTok{plogis}\NormalTok{(}\OperatorTok{-}\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{0} \FloatTok{-0.1}\OperatorTok{*}\NormalTok{w1 }\OperatorTok{+}\StringTok{ }\FloatTok{0.3}\OperatorTok{*}\NormalTok{w2 }\OperatorTok{+}\StringTok{ }\FloatTok{0.25}\OperatorTok{*}\NormalTok{w3 }\OperatorTok{+}\StringTok{ }\FloatTok{0.2}\OperatorTok{*}\NormalTok{w4 }\OperatorTok{+}\StringTok{ }\FloatTok{0.15}\OperatorTok{*}\NormalTok{w2}\OperatorTok{*}\NormalTok{w4))}
  \CommentTok{# Observed outcome}
\NormalTok{  Y <-}\StringTok{ }\NormalTok{Y}\FloatTok{.1}\OperatorTok{*}\NormalTok{A }\OperatorTok{+}\StringTok{ }\NormalTok{Y}\FloatTok{.0}\OperatorTok{*}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{A)}
  \CommentTok{# return data.frame}
  \KeywordTok{data.frame}\NormalTok{(w1, w2, w3, w4, A, Y)}
\NormalTok{\}}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7777}\NormalTok{)}
\NormalTok{n <-}\StringTok{ }\DecValTok{10000}
\NormalTok{Obsdata <-}\StringTok{ }\KeywordTok{generateData}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ n)}
\NormalTok{Obsdata <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Obsdata)}
\NormalTok{Obsdata <-}\KeywordTok{subset}\NormalTok{(Obsdata, }\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(w1, w2, w3, w4, A, Y))}
\NormalTok{Y  <-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{Y}
\NormalTok{A  <-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{A}
\NormalTok{w1 <-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{w1}
\NormalTok{w2 <-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{w2}
\NormalTok{w3 <-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{w3}
\NormalTok{w4 <-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{w4}
\end{Highlighting}
\end{Shaded}

We are going to contrast one-year mortality risk for a population where
all patients were treated with radiotherapy versus another population
where patients were treated with chemotherapy. When estimating the
marginal causal risk ratio (CRR) for a binary treatment (or exposure),
methods that incorporate propensity scores, the G-computation, or a
combination of both, namely double-robust methods, are preferred over
naïve regression approaches which are biased under misspecification of a
parametric outcome model. The Augmented Inverse Probability of Treatment
Weighting (\textbf{AIPTW}) estimation is a double-robust two-step
procedure with two equations (propensity score and mean outcome
equations). Double-robust methods stem from based on semi-parametric
theory (i.e., estimation equations) and only require the correct
specification of one model. Here below we provide the formula to compute
the MCRR:

\[\text{Marginal Causal RR}^{AIPTW}\,=\,\frac{EY1}{EY0}\,=\,\text{log(EY1)}\,-\,\text{log(EY0)}.\]
Where
\[\text{EY(1)}\,=\,\frac{1}{n}\sum_{i=1}^{n}\left(\frac{I\left(A_{i}=1\right)}{g_n(1|W_{i})}\right)\left[Y_{i}-\bar{Q}_{n}\left(A_{i},W_{i}\right)\right]+\frac{1}{n}\sum_{i=1}^{n}\bar{Q}_{n}\left(1,\ W_{i}\right),\]
and
\[\text{EY(0)}\,=\,\frac{1}{n}\sum_{i=1}^{n}\left(\frac{I\left(A_{i}=1\right)}{g_n(0|W_{i})}\right)\left[Y_{i}-\bar{Q}_{n}\left(A_{i},W_{i}\right)\right]+\frac{1}{n}\sum_{i=1}^{n}\bar{Q}_{n}\left(0,\ W_{i}\right).\]
Let\('\)s use R to estimate the CRR. Note that the CRR is a function of
other two functions, the marginal predictions of the regression
coefficients from the propensity score logistic model and the marginal
predictions from the outcome logistic regression model in case of a
binary treatment and outcome.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Step 1 estimation and prediction of the model for the outcome (G-computation)}
\NormalTok{gm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{A }\OperatorTok{+}\StringTok{ }\NormalTok{w1 }\OperatorTok{+}\StringTok{ }\NormalTok{w2 }\OperatorTok{+}\StringTok{ }\NormalTok{w3 }\OperatorTok{+}\StringTok{ }\NormalTok{w4, }\DataTypeTok{data =}\NormalTok{ Obsdata, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link=}\NormalTok{logit))}
\CommentTok{# Prediction for E(Y|A,W), and E(Y|A = 1, W) and, E(Y|A = 0, W)}
\NormalTok{QAW <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(gm, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{Q1W <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(gm, }\DataTypeTok{newdata=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{A =} \DecValTok{1}\NormalTok{, w1, w2, w3, w4), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{Q0W <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(gm, }\DataTypeTok{newdata=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{A =} \DecValTok{0}\NormalTok{, w1, w2, w3, w4), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{# Step 2 estimation and prediction of the propensity score (ps): E(A|W) or E(cmbd|age)}
\NormalTok{psm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(A }\OperatorTok{~}\StringTok{ }\NormalTok{w1 }\OperatorTok{+}\StringTok{ }\NormalTok{w2 }\OperatorTok{+}\StringTok{ }\NormalTok{w3 }\OperatorTok{+}\StringTok{ }\NormalTok{w4, }\DataTypeTok{family =}\NormalTok{ binomial, }\DataTypeTok{data =}\NormalTok{ Obsdata)}
\NormalTok{gW =}\StringTok{ }\KeywordTok{predict}\NormalTok{(psm, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{# CRR}
\NormalTok{EY1 <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((Obsdata}\OperatorTok{$}\NormalTok{Y) }\OperatorTok{*}\StringTok{ }\NormalTok{(Obsdata}\OperatorTok{$}\NormalTok{Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q1W) }\OperatorTok{/}\StringTok{ }\NormalTok{gW }\OperatorTok{+}\StringTok{ }\NormalTok{Q1W)}
\NormalTok{EY0 <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{Y) }\OperatorTok{*}\StringTok{ }\NormalTok{(Obsdata}\OperatorTok{$}\NormalTok{Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q0W) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{gW) }\OperatorTok{+}\StringTok{ }\NormalTok{Q0W)}
\NormalTok{logCRR <-}\StringTok{ }\KeywordTok{log}\NormalTok{(EY1) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(EY0) }
\NormalTok{CRR <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(logCRR);CRR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.156
\end{verbatim}

The marginal contrast of a population of patients where all of them were
treated wich chemotherapy versus other where patients were treated with
radiotherapy after standardizing by the levels of patients age, TNM
stage, comoribidities and gender for the one-year mortality risk is 8
times higher for patients treated with chemoterapy. However, we would
like to get statistical inference for the estimate of the MCRR. Often
researchers use the bootstrap procedure to derive 95\% confidence
intervals (CI) but computing efficiency is low. However, we can use the
functional Delta Method to apply the Central Limit Theorem and derive
Wald type 95\% CI for the CRR. Here we demonstrate how to derive the
functional Delta Method based on the estimation of the efficient
influence curve for the AIPTW estimator based on the formula (3) (Laan
and Rose, 2011). In summary, for observed data \(\text{O}_i\), i = 1,
\ldots, n an asymptotically linear estimator \(\hat\Psi\) of an estimand
\(\Psi\), is an estimator that can be represented as follows (Laan and
Rose, 2011):
\[\hat\Psi\,-\,\Psi\;=\;\frac{1}{n}\sum_{i=1}^n \text{D}(\text{O}_{i})\,+\,\text{Op}(\frac{1}{\sqrt(n)})\;\;(4).\]
Where\\
\[\text{D}(\text{O}_{i})=f'(\theta)(\hat\theta\,-\,\theta).\]

Therefore for the log(MCRR) = log(EY1)-log(EY0), we have the following
two partial derivatives:
\[\frac{\partial log(EY1,EY0)}{\partial EY1}\,=\, \frac{1}{EY1}, \] and,
\[\frac{\partial log(EY0,EY1)}{\partial EY0}\,=\, \frac{1}{EY0}. \]
Let\('\)s now call D1 and D0 the two functionals for the potential
otucomes EY1 and EY0. Thus, based on \((\hat\theta\,-\,\theta)\) we get
(Laan and Rose, 2011):
\[\text{D1} = \left(\left(\frac{I\left(A_{i}=1\right)}{g_n(1|W_{i})}\right)\left(Y_{i}-{Q}_{n}\left(A_{i},W_{i}\right)\right)\,+\,\text{Q}_{n}\left(1,\ W_{i}\right)\right)\,-\,\text{E[Y(1)]}\]
and,
\[\text{D0} = \left(\left(\frac{I\left(A_{i}=0\right)}{g_n(0|W_{i})}\right)\left(Y_{i}-{Q}_{n}\left(A_{i},W_{i}\right)\right)\,+\,\text{Q}_{n}\left(0,\ W_{i}\right)\right)\,-\,\text{E[Y(0)]}\]
Where \(g_{n}\) is the propensity score for the treatment mechanism (A)
and \(\bar Q_{n}\) is the estimate of the conditional mean of outcome
given treatment and confounders (W); \(E(Y|A,W)\). Therefore
D(\(\text{O}_{i}\)) for the
log(MCRR)\(\,=\,\frac{1}{EY1}D1\,+\,\frac{1}{EY0)}D0.\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Functional Delta Method based on the IC for statisical inference}
\NormalTok{D1 <-}\StringTok{ }\NormalTok{(A) }\OperatorTok{*}\StringTok{ }\NormalTok{(Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q1W) }\OperatorTok{/}\StringTok{ }\NormalTok{gW }\OperatorTok{+}\StringTok{ }\NormalTok{Q1W }\OperatorTok{-}\StringTok{ }\NormalTok{EY1 }
\NormalTok{D0 <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{A) }\OperatorTok{*}\StringTok{ }\NormalTok{(Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q0W) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{gW) }\OperatorTok{+}\StringTok{ }\NormalTok{Q0W }\OperatorTok{-}\StringTok{ }\NormalTok{EY0}
\CommentTok{# AIPTW CRR and 95%CI}
\NormalTok{EIC <-}\StringTok{ }\NormalTok{((}\DecValTok{1} \OperatorTok{/}\StringTok{ }\NormalTok{EY1) }\OperatorTok{*}\StringTok{ }\NormalTok{D1) }\OperatorTok{+}\StringTok{ }\NormalTok{((}\DecValTok{1} \OperatorTok{/}\StringTok{ }\NormalTok{EY0) }\OperatorTok{*}\StringTok{ }\NormalTok{D0)}
\NormalTok{varEIC <-}\StringTok{ }\KeywordTok{var}\NormalTok{(EIC) }\OperatorTok{/}\StringTok{ }\NormalTok{n}
\NormalTok{CI <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{exp}\NormalTok{(logCRR }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(varEIC)), }\KeywordTok{exp}\NormalTok{(logCRR }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(varEIC)))}
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ CRR:"}\NormalTok{, CRR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  CRR: 8.156
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ CRR 95%CI:"}\NormalTok{, CI)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  CRR 95%CI: 7.109 9.359
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Summary IC}
\KeywordTok{hist}\NormalTok{(IC)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DeltaMethodTutorial_files/figure-latex/unnamed-chunk-17-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{varEIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.004921
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# gbounds <- function(x, bounds = c(0,1))\{}
 \CommentTok{# x[x<min(bounds)] <- min(bounds)}
  \CommentTok{#x[x>max(bounds)] <- max(bounds)}
  \CommentTok{#return(x)}
\CommentTok{#\}}

\CommentTok{# gWb <- gbounds(gW, bounds=c(0.1,0.90))}

\NormalTok{EY1 <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((Obsdata}\OperatorTok{$}\NormalTok{Y) }\OperatorTok{*}\StringTok{ }\NormalTok{(Obsdata}\OperatorTok{$}\NormalTok{Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q1W) }\OperatorTok{/}\StringTok{ }\NormalTok{gW }\OperatorTok{+}\StringTok{ }\NormalTok{Q1W)}
\NormalTok{EY0 <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{Y) }\OperatorTok{*}\StringTok{ }\NormalTok{(Obsdata}\OperatorTok{$}\NormalTok{Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q0W) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{gW) }\OperatorTok{+}\StringTok{ }\NormalTok{Q0W)}
\NormalTok{logCRR <-}\StringTok{ }\KeywordTok{log}\NormalTok{(EY1) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(EY0) }
\NormalTok{CRR <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(logCRR);CRR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.156
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{D1 <-}\StringTok{ }\NormalTok{(Obsdata}\OperatorTok{$}\NormalTok{A) }\OperatorTok{*}\StringTok{ }\NormalTok{(Obsdata}\OperatorTok{$}\NormalTok{Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q1W) }\OperatorTok{/}\StringTok{ }\NormalTok{gW }\OperatorTok{+}\StringTok{ }\NormalTok{Q1W }\OperatorTok{-}\StringTok{ }\NormalTok{EY1 }
\NormalTok{D0 <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{Obsdata}\OperatorTok{$}\NormalTok{A) }\OperatorTok{*}\StringTok{ }\NormalTok{(Obsdata}\OperatorTok{$}\NormalTok{Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q0W) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{gW) }\OperatorTok{+}\StringTok{ }\NormalTok{Q0W }\OperatorTok{-}\StringTok{ }\NormalTok{EY0}
\CommentTok{# AIPTW CRR and 95%CI}
\NormalTok{IC <-}\StringTok{ }\NormalTok{((}\DecValTok{1} \OperatorTok{/}\StringTok{ }\NormalTok{EY1) }\OperatorTok{*}\StringTok{ }\NormalTok{D1) }\OperatorTok{+}\StringTok{ }\NormalTok{((}\DecValTok{1} \OperatorTok{/}\StringTok{ }\NormalTok{EY0) }\OperatorTok{*}\StringTok{ }\NormalTok{D0)}
\NormalTok{varHat <-}\StringTok{ }\KeywordTok{var}\NormalTok{(IC) }\OperatorTok{/}\StringTok{ }\NormalTok{n}
\NormalTok{CI <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{exp}\NormalTok{(logCRR }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(varHat)), }\KeywordTok{exp}\NormalTok{(logCRR }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(varHat))); CRR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.156
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ CRR:"}\NormalTok{, CRR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  CRR: 8.156
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ CRR 95%CI:"}\NormalTok{, CI)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  CRR 95%CI: 7.109 9.359
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Summary IC}
\KeywordTok{hist}\NormalTok{(IC)}
\end{Highlighting}
\end{Shaded}

\includegraphics{DeltaMethodTutorial_files/figure-latex/unnamed-chunk-18-1.pdf}

Note: In general, when we have near positivity violations i.e., the
positivity assumption states that within strata of W (age in our
example) every patient had a nonzero probability of receiving either of
the two treatment conditions (i.e.~0
\textless P(A=1\textbar W)\textless1) the Bootstrap is preferred.

\hypertarget{the-bootstrap}{%
\section{The Bootstrap}\label{the-bootstrap}}

Finally, we are going to use a more conservative approach for
statistical inference (i.e., the bootstrap). Using the Bootstrap the
original sample approximates the population from which it was drawn. So
resamples from this sample approximate what we would get if we took many
samples from the population. The bootstrap distribution of a statistic,
based on many resamples, approximates the sampling distribution of the
statistic, based on many samples (Efron and Efron, 1982; Efron and Gong,
1983). For statistical inference, we use the bootstrap standard error of
a statistic is the standard deviation of the bootstrap distribution of
that statistic. For most statistics, bootstrap distributions approximate
the spread, bias, and shape of the actual sampling distribution. The
interval between 2.5 and 97.5 percentiles of the bootstrap distribution
of a statistic is a 95\% bootstrap percentile confidence interval for
the corresponding parameter (Efron and Efron, 1982; Efron and Gong,
1983).

Now let\('\)s use the \textbf{boot} R package which provides extensive
facilities for bootstrapping and related resampling methods. You can
bootstrap a single statistic (e.g.~a median), or a vector (e.g.,
regression weights). We are going to use the nonparametric bootstrapping
(Canty, 2002).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Can get original estimate, by plugging in indices 1:n}
\KeywordTok{library}\NormalTok{(boot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'boot'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:car':
## 
##     logit
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:msm':
## 
##     cav
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aiptw.w =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data,indices)}
\NormalTok{\{}
\NormalTok{    dat=Obsdata[indices,]}
    \CommentTok{# CRR}
\NormalTok{    gm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{A }\OperatorTok{+}\StringTok{ }\NormalTok{w1 }\OperatorTok{+}\StringTok{ }\NormalTok{w2 }\OperatorTok{+}\StringTok{ }\NormalTok{w3 }\OperatorTok{+}\StringTok{ }\NormalTok{w4, }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link=}\NormalTok{logit))}
    \CommentTok{# Prediction for E(Y|A,W), and E(Y|cmbd = 1, age) and, E(Y|cmbd = 0, age)}
\NormalTok{    Q1W <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(gm, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{A =} \DecValTok{1}\NormalTok{, w1, w2, w3, w4), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{    Q0W <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(gm, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{A =} \DecValTok{0}\NormalTok{, w1, w2, w3, w4), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
    \CommentTok{# Step 2 estimation and prediction of the propensity score (ps): E(A|W) or E(cmbd|age)}
\NormalTok{    psm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(A }\OperatorTok{~}\StringTok{ }\NormalTok{w1 }\OperatorTok{+}\StringTok{ }\NormalTok{w2 }\OperatorTok{+}\StringTok{ }\NormalTok{w3 }\OperatorTok{+}\StringTok{ }\NormalTok{w4, }\DataTypeTok{family =}\NormalTok{ binomial, }\DataTypeTok{data =}\NormalTok{ dat)}
\NormalTok{    gW =}\StringTok{ }\KeywordTok{predict}\NormalTok{(psm, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
    \CommentTok{# CRR}
\NormalTok{    EY1 <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((Y) }\OperatorTok{*}\StringTok{ }\NormalTok{(Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q1W) }\OperatorTok{/}\StringTok{ }\NormalTok{gW }\OperatorTok{+}\StringTok{ }\NormalTok{Q1W)}
\NormalTok{    EY0 <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{Y) }\OperatorTok{*}\StringTok{ }\NormalTok{(Y }\OperatorTok{-}\StringTok{ }\NormalTok{Q0W) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{gW) }\OperatorTok{+}\StringTok{ }\NormalTok{Q0W)}
\NormalTok{    CRR <-}\StringTok{ }\NormalTok{EY1 }\OperatorTok{/}\StringTok{ }\NormalTok{EY0}
\NormalTok{\}}
\CommentTok{# Can get original estimate, by plugging in indices 1:n}
\KeywordTok{aiptw.w}\NormalTok{(data,}\DataTypeTok{indices=}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data))}
\CommentTok{# Draw 200 bootstrap sample estimates }
\NormalTok{boot.out=}\KeywordTok{boot}\NormalTok{(data,aiptw.w,}\DecValTok{1000}\NormalTok{)}
\CommentTok{# compute confidence intervals using percentile method}
\KeywordTok{boot.ci}\NormalTok{(boot.out,}\DataTypeTok{type=}\StringTok{"perc"}\NormalTok{,}\DataTypeTok{conf=}\FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot.out, conf = 0.95, type = "perc")
## 
## Intervals : 
## Level     Percentile     
## 95%   ( 6.277, 19.633 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

Note that we got slightly narrower confidence intervals using the delta
method compared to the Bootstrap procedure. It is because with the
functional delta method in finite samples for causal inference the
coverage decreases when there are violations of the positivity
violations and it provide more precisse confidence intervals than other
approaches such as the Bootastrap (Dorie \emph{et al.}, 2019). In our
empirical example the bootstrap compared to the delta method provide
more conservative confidence intervals.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The Delta Method is widely used in classical epidemiological methods
  to derive the standard error (i.e., statistical inference) of
  functions of the coefficients of the parameters fitted in regression
  models. In casual inference is largely used because it eases the
  derivation of Wald type confidence intervals for data-adaptive
  double-robust estimators.
\item
  Recently, the study of the behaviour of the delta method for
  data-adaptive double-robust estimators in finite samples where there
  are positivity violations has shown that the functional delta method
  provides less conservative confidence intervals thant the Bootstrap.
  Simulations have shown that in such situations the coverage is poor
  (Dorie \emph{et al.}, 2019). Therefore, more conservative and robust
  approaches such as the bootstrap procedure is still a valid and
  preferred method for statistical inference under violations and near
  violations of the positivity assumption.
\item
  Users wanted to implemeted the bootstrap proceure using Targeted
  Maximum Likelihood Estimation would like to use the bootstrap TMLE
  package: \url{https://github.com/wilsoncai1992/TMLEbootstrap}
\end{enumerate}

\hypertarget{session-info}{%
\section{Session Info}\label{session-info}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{session_info}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## - Session info ---------------------------------------------------------------
##  setting  value                       
##  version  R version 3.6.1 (2019-07-05)
##  os       Windows 10 x64              
##  system   x86_64, mingw32             
##  ui       RTerm                       
##  language (EN)                        
##  collate  Spanish_Spain.1252          
##  ctype    Spanish_Spain.1252          
##  tz       Europe/Paris                
##  date     2020-04-24                  
## 
## - Packages -------------------------------------------------------------------
##  package      * version    date       lib source                            
##  abind          1.4-5      2016-07-21 [1] CRAN (R 3.6.0)                    
##  acepack        1.4.1      2016-10-29 [1] CRAN (R 3.6.1)                    
##  assertthat     0.2.1      2019-03-21 [1] CRAN (R 3.6.1)                    
##  backports      1.1.5      2019-10-02 [1] CRAN (R 3.6.1)                    
##  base64enc      0.1-3      2015-07-28 [1] CRAN (R 3.6.0)                    
##  boot         * 1.3-24     2019-12-20 [1] CRAN (R 3.6.1)                    
##  broom          0.5.5      2020-02-29 [1] CRAN (R 3.6.3)                    
##  callr          3.4.1      2020-01-24 [1] CRAN (R 3.6.1)                    
##  car          * 3.0-6      2019-12-23 [1] CRAN (R 3.6.2)                    
##  carData      * 3.0-3      2019-11-16 [1] CRAN (R 3.6.1)                    
##  cellranger     1.1.0      2016-07-27 [1] CRAN (R 3.6.1)                    
##  checkmate      1.9.4      2019-07-04 [1] CRAN (R 3.6.1)                    
##  class          7.3-15     2019-01-01 [1] CRAN (R 3.6.1)                    
##  cli            2.0.1      2020-01-08 [1] CRAN (R 3.6.1)                    
##  cluster        2.1.0      2019-06-19 [1] CRAN (R 3.6.2)                    
##  colorspace     1.4-1      2019-03-18 [1] CRAN (R 3.6.1)                    
##  crayon         1.3.4      2017-09-16 [1] CRAN (R 3.6.1)                    
##  curl           4.3        2019-12-02 [1] CRAN (R 3.6.1)                    
##  data.table     1.12.8     2019-12-09 [1] CRAN (R 3.6.2)                    
##  DBI            1.1.0      2019-12-15 [1] CRAN (R 3.6.1)                    
##  dbplyr         1.4.2      2019-06-17 [1] CRAN (R 3.6.1)                    
##  desc           1.2.0      2018-05-01 [1] CRAN (R 3.6.1)                    
##  devtools       2.2.2      2020-02-17 [1] CRAN (R 3.6.3)                    
##  digest         0.6.23     2019-11-23 [1] CRAN (R 3.6.1)                    
##  dplyr        * 0.8.4      2020-01-31 [1] CRAN (R 3.6.1)                    
##  e1071          1.7-3      2019-11-26 [1] CRAN (R 3.6.1)                    
##  ellipsis       0.3.0      2019-09-20 [1] CRAN (R 3.6.1)                    
##  evaluate       0.14       2019-05-28 [1] CRAN (R 3.6.1)                    
##  expm           0.999-4    2019-03-21 [1] CRAN (R 3.6.3)                    
##  fansi          0.4.1      2020-01-08 [1] CRAN (R 3.6.1)                    
##  forcats      * 0.4.0      2019-02-17 [1] CRAN (R 3.6.1)                    
##  foreign        0.8-75     2020-01-20 [1] CRAN (R 3.6.1)                    
##  Formula        1.2-3      2018-05-03 [1] CRAN (R 3.6.1)                    
##  fs             1.3.1      2019-05-06 [1] CRAN (R 3.6.1)                    
##  generics       0.0.2      2018-11-29 [1] CRAN (R 3.6.1)                    
##  ggplot2      * 3.2.1      2019-08-10 [1] CRAN (R 3.6.2)                    
##  glue           1.3.1      2019-03-12 [1] CRAN (R 3.6.1)                    
##  gridExtra      2.3        2017-09-09 [1] CRAN (R 3.6.1)                    
##  gtable         0.3.0      2019-03-25 [1] CRAN (R 3.6.1)                    
##  haven          2.2.0      2019-11-08 [1] CRAN (R 3.6.1)                    
##  Hmisc          4.3-0      2019-11-07 [1] CRAN (R 3.6.1)                    
##  hms            0.5.3      2020-01-08 [1] CRAN (R 3.6.1)                    
##  htmlTable      1.13.3     2019-12-04 [1] CRAN (R 3.6.1)                    
##  htmltools      0.4.0.9002 2020-01-21 [1] Github (rstudio/htmltools@e07546c)
##  htmlwidgets    1.5.1      2019-10-08 [1] CRAN (R 3.6.2)                    
##  httr           1.4.1      2019-08-05 [1] CRAN (R 3.6.1)                    
##  jpeg           0.1-8.1    2019-10-24 [1] CRAN (R 3.6.1)                    
##  jsonlite       1.6.1      2020-02-02 [1] CRAN (R 3.6.1)                    
##  knitr          1.27       2020-01-16 [1] CRAN (R 3.6.2)                    
##  lattice        0.20-38    2018-11-04 [1] CRAN (R 3.6.1)                    
##  latticeExtra   0.6-29     2019-12-19 [1] CRAN (R 3.6.1)                    
##  lazyeval       0.2.2      2019-03-15 [1] CRAN (R 3.6.1)                    
##  lifecycle      0.1.0      2019-08-01 [1] CRAN (R 3.6.1)                    
##  lubridate      1.7.4      2018-04-11 [1] CRAN (R 3.6.1)                    
##  magrittr       1.5        2014-11-22 [1] CRAN (R 3.6.1)                    
##  MASS           7.3-51.5   2019-12-20 [1] CRAN (R 3.6.1)                    
##  Matrix         1.2-18     2019-11-27 [1] CRAN (R 3.6.1)                    
##  memoise        1.1.0      2017-04-21 [1] CRAN (R 3.6.1)                    
##  modelr         0.1.5      2019-08-08 [1] CRAN (R 3.6.1)                    
##  msm          * 1.6.8      2019-12-16 [1] CRAN (R 3.6.3)                    
##  munsell        0.5.0      2018-06-12 [1] CRAN (R 3.6.1)                    
##  mvtnorm        1.0-12     2020-01-09 [1] CRAN (R 3.6.1)                    
##  nlme           3.1-143    2019-12-10 [1] CRAN (R 3.6.1)                    
##  nnet           7.3-12     2016-02-02 [1] CRAN (R 3.6.1)                    
##  nortest        1.0-4      2015-07-30 [1] CRAN (R 3.6.0)                    
##  openxlsx       4.1.4      2019-12-06 [1] CRAN (R 3.6.1)                    
##  pillar         1.4.3      2019-12-20 [1] CRAN (R 3.6.1)                    
##  pkgbuild       1.0.6      2019-10-09 [1] CRAN (R 3.6.1)                    
##  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 3.6.1)                    
##  pkgload        1.0.2      2018-10-29 [1] CRAN (R 3.6.1)                    
##  png            0.1-7      2013-12-03 [1] CRAN (R 3.6.0)                    
##  prettyunits    1.1.1      2020-01-24 [1] CRAN (R 3.6.1)                    
##  processx       3.4.1      2019-07-18 [1] CRAN (R 3.6.1)                    
##  ps             1.3.0      2018-12-21 [1] CRAN (R 3.6.1)                    
##  purrr        * 0.3.3      2019-10-18 [1] CRAN (R 3.6.1)                    
##  R6             2.4.1      2019-11-12 [1] CRAN (R 3.6.1)                    
##  RcmdrMisc    * 2.7-0      2020-01-14 [1] CRAN (R 3.6.2)                    
##  RColorBrewer   1.1-2      2014-12-07 [1] CRAN (R 3.6.0)                    
##  Rcpp           1.0.3      2019-11-08 [1] CRAN (R 3.6.1)                    
##  readr        * 1.3.1      2018-12-21 [1] CRAN (R 3.6.1)                    
##  readxl         1.3.1      2019-03-13 [1] CRAN (R 3.6.1)                    
##  remotes        2.1.1      2020-02-15 [1] CRAN (R 3.6.3)                    
##  reprex         0.3.0      2019-05-16 [1] CRAN (R 3.6.1)                    
##  rio            0.5.16     2018-11-26 [1] CRAN (R 3.6.1)                    
##  rlang          0.4.4      2020-01-28 [1] CRAN (R 3.6.1)                    
##  rmarkdown      2.1.1      2020-01-28 [1] Github (rstudio/rmarkdown@18ba267)
##  rpart          4.1-15     2019-04-12 [1] CRAN (R 3.6.1)                    
##  rprojroot      1.3-2      2018-01-03 [1] CRAN (R 3.6.1)                    
##  rstudioapi     0.11       2020-02-07 [1] CRAN (R 3.6.2)                    
##  rvest          0.3.5      2019-11-08 [1] CRAN (R 3.6.1)                    
##  sandwich     * 2.5-1      2019-04-06 [1] CRAN (R 3.6.1)                    
##  scales         1.1.0      2019-11-18 [1] CRAN (R 3.6.1)                    
##  sessioninfo    1.1.1      2018-11-05 [1] CRAN (R 3.6.1)                    
##  stringi        1.4.5      2020-01-11 [1] CRAN (R 3.6.1)                    
##  stringr      * 1.4.0      2019-02-10 [1] CRAN (R 3.6.3)                    
##  survival       3.1-8      2019-12-03 [1] CRAN (R 3.6.1)                    
##  testthat       2.3.1      2019-12-01 [1] CRAN (R 3.6.2)                    
##  tibble       * 2.1.3      2019-06-06 [1] CRAN (R 3.6.1)                    
##  tidyr        * 1.0.2      2020-01-24 [1] CRAN (R 3.6.1)                    
##  tidyselect     1.0.0      2020-01-27 [1] CRAN (R 3.6.1)                    
##  tidyverse    * 1.3.0      2019-11-21 [1] CRAN (R 3.6.1)                    
##  usethis        1.5.1      2019-07-04 [1] CRAN (R 3.6.1)                    
##  vctrs          0.2.2      2020-01-24 [1] CRAN (R 3.6.1)                    
##  withr          2.1.2      2018-03-15 [1] CRAN (R 3.6.1)                    
##  xfun           0.12       2020-01-13 [1] CRAN (R 3.6.1)                    
##  xml2           1.2.2      2019-08-09 [1] CRAN (R 3.6.1)                    
##  yaml           2.2.1      2020-02-01 [1] CRAN (R 3.6.1)                    
##  zip            2.0.4      2019-09-01 [1] CRAN (R 3.6.1)                    
##  zoo            1.8-7      2020-01-10 [1] CRAN (R 3.6.1)                    
## 
## [1] C:/Users/dredondo/Documents/R/R-3.6.1/library
\end{verbatim}

\hypertarget{thank-you}{%
\section{Thank you}\label{thank-you}}

Thank you for participating in this tutorial.\\
If you have updates or changes that you would like to make, please send
me a pull request. Alternatively, if you have any questions, please
e-mail me. You can cite this repository as:\\
Luque-Fernandez MA, (2019). Delta Method in Epidemiology: An Applied and
Reproducible Tutorial. GitHub repository,
\url{http://migariane.github.io/DeltaMethodEpi.nb.html}.\\
\textbf{Miguel Angel Luque Fernandez}\\
\textbf{E-mail:} \emph{miguel-angel.luque at lshtm.ac.uk}\\
\textbf{Twitter} \texttt{@WATZILEI}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Agresti2010}{}%
Agresti A. (2010). Analysis of ordinal categorical data. 2nd ed. Wiley:
Hoboken, N.J.

\leavevmode\hypertarget{ref-Armi2005}{}%
Armitage P, Colton T. (2005). Encyclopedia of biostatistics. 2nd ed.
John Wiley: Chichester, West Sussex, England.

\leavevmode\hypertarget{ref-Boos2013}{}%
Boos DD, Stefanski LA. (2013). Essential statistical inference: Theory
and methods. Springer.

\leavevmode\hypertarget{ref-boostrap2003}{}%
Canty A. (2002). Resampling methods in r: The boot package. \emph{R
News} \textbf{2}: 2--7.

\leavevmode\hypertarget{ref-Dorie_2019}{}%
Dorie V, Hill J, Shalit U, Scott M, Cervone D. (2019). Rejoinder:
Response to discussions and a look ahead. \emph{Statistical Science}
\textbf{34}: 94--99.

\leavevmode\hypertarget{ref-efron1982}{}%
Efron B, Efron B. (1982). The jackknife, the bootstrap and other
resampling plans. SIAM.

\leavevmode\hypertarget{ref-efron1983}{}%
Efron B, Gong G. (1983). A leisurely look at the bootstrap, the
jackknife, and cross-validation. \emph{The American Statistician}
\textbf{37}: 36--48.

\leavevmode\hypertarget{ref-Efron1993}{}%
Efron B, Tibshirani R. (1993). An introduction to the bootstrap. Chapman
\& Hall: New York.

\leavevmode\hypertarget{ref-robins1986}{}%
Greenland S, Robins JM. (1986). Identifiability, exchangeability, and
epidemiological confounding. \emph{International journal of
epidemiology} \textbf{15}: 413--419.

\leavevmode\hypertarget{ref-Gutman2015}{}%
Gutman R, Rubin DB. (2015). Estimation of causal effects of binary
treatments in unconfounded studies. \emph{Stat Med} \textbf{34}:
3381--98.

\leavevmode\hypertarget{ref-Herberg1962}{}%
Herberg T, Bristol JD. (1962). Elementary mathematical analysis. Heath:
Boston.

\leavevmode\hypertarget{ref-kavroudakis2015}{}%
Kavroudakis D, others. (2015). Sms: An r package for the construction of
microdata for geographical analysis. \emph{Journal of Statistical
Software} \textbf{68}.

\leavevmode\hypertarget{ref-kennedy2016}{}%
Kennedy EH. (2016). Semiparametric theory and empirical processes in
causal inference. In: \emph{Statistical causal inferences and their
applications in public health research}. Springer, pp 141--167.

\leavevmode\hypertarget{ref-kennedy2017}{}%
Kennedy EH, Ma Z, McHugh MD, Small DS. (2017). Non-parametric methods
for doubly robust estimation of continuous treatment effects.
\emph{Journal of the Royal Statistical Society: Series B (Statistical
Methodology)} \textbf{79}: 1229--1245.

\leavevmode\hypertarget{ref-van2011}{}%
Laan M van der, Rose S. (2011). Targeted learning: Causal inference for
observational and experimental data. Springer Series in Statistics.

\leavevmode\hypertarget{ref-Rothman2008}{}%
Rothman KJ, Greenland S, Lash TL. (2008). Modern epidemiology. 3rd ed.,
thoroughly rev. and updated. Wolters Kluwer Health/Lippincott Williams
\& Wilkins: Philadelphia.

\leavevmode\hypertarget{ref-Rubin2007}{}%
Rubin DB. (2007). The design versus the analysis of observational
studies for causal effects: Parallels with the design of randomized
trials. \emph{Stat Med} \textbf{26}: 20--36.

\end{document}
