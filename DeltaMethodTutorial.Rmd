---
title: "Statistical Inference for Functionals in Epidemiology using the Delta Method and Influence Functions: An Applied and Reproducible Tutorial"
author: "Miguel Angel Luque Fernandez, MA, MPH, MSc, Ph.D"
date: "02/03/2020 \n https://scholar.harvard.edu/malf/home"
output:
  pdf_document:
     toc: true
     number_sections: true
  word_document:
    toc: yes
  html_notebook:
    code_folding: show
    highlight: default
    number_sections: no
    theme: journal
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
  html_document:
    toc: yes
font-family: Risque
font-import: http://fonts.googleapis.com/css?family=Risque
csl: references/isme.csl
bibliography: references/bibliography.bib
---

<a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-show-count="false">Tweet</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

```{r options, echo=FALSE, eval=FALSE}
options(scipen=999, digits=5, tinytex.verbose = TRUE)
```

# Introduction

A fundamental problem in inferential statistics is to approximate the distribution of a statistic calculated from a probability sample of data. The statistic is usally a parameter estimate that characterizes the sampling variability of the estimate [@Boos2013]. Becasue the number and type of inference problems for which exact parametric distributions can be determined is limited the approximation of the distribution of the statistic is necesary. Approximate inference via determination of the asymptotic distribution of the statistics is a techique that epidemiologists and applied statisticians, we have been using for years. We name it the "Delta Method". The delta method is a theorem which states that a smooth function of an asymptotically normal estimator is also asymptotically normally distributed. It can also be viewed as a technique for approximating variance of a functional i.e., a nonlinear function of a random variable that can be approximate by averages [@Armi2005; @Boos2013]. 
 
For instance, in epidemiology we use routinely the delta method to compute the standard error (SE) of functionals as the risk difference (RD), the risk ratio (RR), and the odds ratio (OR) [@Agresti2010]. Often in addition to reporting parameters fit by a model, we need to report some marginal transformation of these parameters. The transformation can generate the point estimates of our desired values, but the SE of these point estimates are not so easily calculated and we need to approximate the distribution of the SE for statistical inference using the delta method. 

More recently, in causal inference epidemiologists compute the average treatment effect (ATE) using G-computation, a generalization of the standardization, among other estimations techinques as the inverse probability of treatment weights [@Rubin2007]. The ATE is defined by an average which is a function of two random variables (i.e., the potential outcomes E(Y1) and E(Y0)) [@Rubin2007; @Gutman2015]. These two random variables are predicted from the coefficients of the parameters fitted in two different regression models [@robins1986]. The functional delta method applied to an estimator implicitly defined by averages, also known as M-estimator, is the method used to approximate the variance of the estimator (i.e., G-computation for the ATE) and can be used to estimate the  standar error (SE) of the ATE under some theoretical assumptioms from Semiparametric and Empirical Process theory [@Boos2013; @kennedy2016; @kennedy2017]. We assume that the estimator used to derive the ATE is pathwise differentiable and an asymptotically regular annd linear estimator [@Boos2013; @kennedy2016; @kennedy2017]. We, applied statisticians and epidemiologists should knonw how to determine when a large sample approximation to the distribution of a statistic is appropiate, how to derive the approximation, and how to use it for inference applications. In this tutorial we introduce the use of the functional delta method in Epidemiology from a practical perspective including boxes with code in R and Stata software to allow readers learning by doing. 

# The Delta Method
Note that although the delta method is often appropriate to use with large samples, other methods can be used to estimate standard errors, such as the  bootstrap [@Efron1993; @efron1982]. Essentially, the delta method involves calculating the expansion up to the first order of a Taylor series approximation of a function [@Herberg1962] to derive the empirical value and the SE of an estimator. When a statistic can be approximate by an average, the approximating average is usually an average of some function of the sample values and can be writen in the form: 

$$f(\hat\theta)-f(\theta)\;\approx\;f'(\theta)(\hat\theta\,-\,\theta)\,+\,\text{Op}(\frac{1}{\sqrt(n)}) \;\;(1).$$
Where $f(\hat\theta)$ that can be interpreted as the large sample stochastic limit of $f(\theta)$ and $f'(\theta)(\hat\theta\,-\,\theta)$ is a function often called the Influence Curve and the remaider $\text{Op}(\frac{1}{\sqrt(n)})$ is negligibly small as the sample size increases [@Boos2013] i.e., asymptotically converges to zero,

$$\text{Op}(\frac{1}{\sqrt(n)}) \rightarrow 0 \;,$$
and the distribution of the statistic converges in probability to a normally distrubuted random variable with mean zero and finite variance. 

$$\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,,var(\theta)).$$ 
Therefore, suppose that $(\hat\theta)$ is a multivariable sample mean and $f$ is a smooth scalar valued function. Then, the mean of an estimator $f(\hat\theta)$ can be approximate with the population mean $f(\theta)$ plus the influence curve derived as the first derivative of the estimator times the difference between the sample population mean minus the mean of the estimator approximated as follows: 

$$f(\hat\theta)\approx\;f(\theta)\,+\,f'(\theta)(\hat\theta\,-\,\theta)\, \;\;(2)$$
$$f(\bar{Y})\approx\;f(\mu)\,+\,f'(\mu)(\bar{Y},-\,\mu)\, \;= n^{-1}\sum{f(\mu)\,+\,f'(\mu)(\bar{Y_{i}}\,-\,\mu)},$$
Note that the variance (i.e., standard error of the estimator) is then estimated using the IC (i.e., $f'(\theta)(\hat\theta\,-\,\theta)$) and $f'(\mu)(\bar{Y_{i}}\,-\,\mu)$ for the multivariate sample mean. Now lets suppose that we want to consider a pair of independent identically distributed (iid) multivariable random sample means (i.e., Y1 and Y2) and we want to derive the SE for the ratio estimator $f(\bar{Y})\,=\,\bar{Y_{1}}/\bar{Y_{2}}$ where $\mu_{2}\neq\,0$. In this case the IC for the ratio will be estimated as follows:

$$f'(\mu)\,=\, \frac{1}{\mu_{1}}\,-\,\frac{\mu_{1}}{\mu_{x}^{2}}$$
Then $$IC = \frac{1}{\mu_{1}}{}(\bar{Y_{i1}}\,-\,\mu_{1})\,-\,\frac{\mu_{1}}{\mu_{x}^{2}}(\bar{Y_{i2}}\,-\,\mu_{2})$$ 

For more than two iid multivariable random variables we first get the Taylor series approximation of the function using the first two terms of the expansion of the transformation function about the mean of of the random variable but we need to compute  a vector of partial derivatives of $\theta$ (i.e., $\nabla \theta$ which is the gradient of $\theta$). We can then take the variance of this approximation to estimate the variance of $\theta$ and thus the SE of a transformed parameter. The first two terms of the Taylor expansion are then an approximation for $\hat\theta$ as follows:,

$$f(\hat\theta) \approx\, f(\theta) \,+\,\nabla(\theta)^T\,(\hat\theta\,-\,\theta)\; (3).$$
Where $\nabla \theta$ is the gradient of $\theta$, or a vector of partial derivatives of $\theta$. We can then take the variance of this approximation to estimate the variance of $\theta$ and thus the SE of a transformed parameter. 

In summary, for observed data $\text{O}_i$, i = 1, ..., n an asymptotically linear estimator $\hat\Psi$ of an estimand $\Psi$, is an estimator that can be represented as follows: 
$$\sqrt(n)(\hat\Psi\,-\,\Psi);=\;\frac{1}{n}\sum_{i=1}^n \text{D}(\text{O}_{i})\,+\,\text{Op}(\frac{1}{\sqrt(n)})\;\;(4).$$
Where $\text{D}(\text{O}_{i})$ represents the IC of the estimator:

$$\text{D}(\text{O}_{i})=f'(\theta)(\hat\theta\,-\,\theta).$$
In other words, the difference between the estimator and estimand can be represented as the sample mean of a fixed function (the *"IC”*) plus a remainder term that must converge to 0 at a rate faster than $\frac{1}{\sqrt{n}}$. The estimated IC provides an asymptotic variance estimate for the estimator (i.e., we can apply the *Central Limit Theorem* and compute *Wald* type confidence intervals) [@van2011]. 


# Delta method for the a simple sample mean

```{r, echo=FALSE, warning=FALSE}
# M-estimation for the sample mean
# Data generation
set.seed(7777)
n <- 1000
x <- runif(n,0,1)
mx = mean(x)
# Functional delta-method: influence curve for the sample mean (first derivative=1(constant))
IC = 1 * (x - mx)
# M-estimation of the sample mean
Yhat <- x + IC # Pluging estimator
Yhat <- mean(Yhat); Yhat
# Geometry of the IC
plot(x, IC)
hist(IC)
# Standard Error: Influence Curve
varYhat.IC <- var(IC)/n;
seIC <- sqrt(varYhat.IC)
# Asymptotic linear inference 95% Confidence Intervals
Yhat_CI <- c(mean(Yhat) - 1.96*sqrt(varYhat.IC), mean(Yhat) + 1.96*sqrt(varYhat.IC)); 
mean(Yhat); Yhat_CI
# Check with implemented delta-method in library MSM 
library(msm)
se = deltamethod(g= ~x1, mean= mx, cov= varYhat.IC)
print(se)
seIC
# Check 95%CI delta-method computed by hand with delta-method implemented in RcmdrMisc library
Yhat_CI <- c(mean(Yhat) - 1.96*sqrt(varYhat.IC), mean(Yhat) + 1.96*sqrt(varYhat.IC)); 
mean(Yhat); Yhat_CI
library(RcmdrMisc)
DeltaMethod(lm(x ~ 1), "b0")
```

# Delta method for the most commun epidemiological estimands: Risk differeces, relative risk, and odds ratio

Using the classical 2 by 2 epidemiological table presenting outcome counts by the levels of a risk factor, we are going to derive the SE for the for the RD, RR and OR using the Delta Method

Risk      | Alive  | Dead         
--------  | ----------- | -----------
Exposed   |$\text{n}_{11}$ = ($\text{p}_{1}$) | $\text{n}_{21}$ = ($\text{p}_{2}$)   
Unexposed |$\text{n}_{12}$ = (1 -$\text{p}_{1}$)| $\text{n}_{22}$ = (1-$\text{p}_{2}$) 
  N       | $\text{N}_{1}$     |   $\text{N}_{2}$     

### Risk Difference (RD)
The risk difference is defined as follows [@Rothman2008]:  

$$\widehat{RD} \,=\, \hat \theta_{1}\,-\,\hat \theta_{2} \,=\, \hat p_{1}\,-\,\hat p_{2}.$$
Assuming that the probability of the event (**p**) can be modelled using a Bernoulli distribution with range $0\;\leq \text{p} \leq\;1$, expectation of p is E(p) = p, and the variance var(p) = p(1-p)/n.   

Given that
$$\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,, var(\theta)).$$
and using for the probability (p) the formula in (4)

$$SE(\widehat{RD})\,=f'(\theta)(var(\theta))\;=\; \sqrt{\frac{p(1-p)}{n}}$$
We have that for the RD the SE is  
$$SE(\widehat{RD})\,=\, \sqrt{\frac{(1-\hat p_{1})}{n_{1}}}\,+\,\sqrt{\frac{(1-\hat p_{2})}{n_{2}}}\,=\,\sqrt{\frac{(1-\hat p_{1})}{n_{1}}\,+\,\frac{(1-\hat p_{2})}{n_{2}}}$$

### Risk Ratio (RR) 
$$\widehat{RR} \,=\, \frac{\hat \theta_{1}}{\hat \theta_{2}} \,=\, log\left(\frac{\hat p_{1}}{\hat p_{2}}\right)\,=\,log(\hat p_{1}) + log(\hat p_{2})$$
Given that
$$\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,, var(\theta))$$
then
$$SE(log(\hat{\theta}))\,=f'(\theta)(var(\theta))\;=\;\frac{1}{p}\sqrt{\frac{p(1-p)}{n}}\,=\,\sqrt{\frac{(1-p)}{pn}}$$
We have that
$$SE(log(\widehat{RR}))\,=\, \sqrt{\frac{(1-\hat p_{1})}{\hat p_{1}n_{1}}}\,+\,\sqrt{\frac{(1-\hat p_{2})}{\hat p_{2}n_{2}}}\,=\,\sqrt{\frac{(1-\hat p_{1})}{\hat p_{1}n_{1}}\,+\,\frac{(1-\hat p_{2})}{\hat p_{2}n_{2}}}$$

### Odds Ratio (OR) 
$$\widehat{OR} \,=\, \frac{\hat \theta_{1}}{\hat \theta_{2}} \,=\, log\left(\frac{\hat p_{1}/(1-\hat p_{1})}{\hat p_{2}/(1-\hat p_{2})}\right)\,=\,log\left(\frac{\text{n}_{11}\text{n}_{22}}{\text{n}_{12}\text{n}_{21}}\right)\,$$
Given that
$$\sqrt(n)\left(f(\hat\theta)-f(\theta)\right)\; \rightarrow N(0\,, var(\theta))$$
then
$$SE(log(\hat{\theta}))\,=f'(\theta)(var(\theta))\;=\;\frac{1}{p(1-p)}\sqrt{\frac{p(1-p)}{n}}\,=\,\sqrt{\frac{1}{n}}$$

We have that
$$SE(log(\widehat{OR}))\,=\, \sqrt{\frac{1}{n_{11}}\,+\,\frac{1}{n_{12}}\,+\,\frac{1}{n_{21}}\,+\,\frac{1}{n_{22}}}$$
Finally, using the *Central Limit Theorem* the Wald type 95\% Confidence Intervals for the RD, RR and OR can be estimated as follows:  

$$95\%\text{CI}\,=\,1.96\times\text(SE(\hat\theta))$$

# Empirical example
To illustrate the use of the Delta Method we are going to generate data based on a cancer epidemiology example where we want to estimate the effect of comorbidities (binary indicator) on one-year cancer mortality controlling for the confounding effect of age in a cohort of 1,000 patients in their middle age. We assume that it is an extremely lethal type of cancer (i.e., pancreatic cancer) thus we can expect high one-year mortality rate among younger patients. Age in years was generated as a normal random variable with mean 65 years and standard deviation 5 years. Comorbidities was generated as a binary indicator and as a function of age using a binomial model. Patients$’$ one-year mortality rate was generated as a function of the patients$’$ age and the presence of comorbidities using a binomial distribution. The data generation and models specifications are provide here below: 

```{r, echo=FALSE, warning=FALSE}
# Data generation
library(tidyverse)

generateData <- function(n, seed){
set.seed(seed)
age <- rnorm(n, 65, 5)
cmbd <- rbinom(n, size=1, prob = plogis(1 - 0.05 * age))
Y <- rbinom(n, size=1, prob = plogis(1 - 0.02* age - 0.02 * cmbd))
data.frame(Y, cmbd, age)
}
```

Here we describe the data

```{r}
# Describing the data
data <- generateData(n = 1000, seed = 777) 
str(data)
summarize(
  data,
  Status = mean(Y), 
  Comorbidities = mean(cmbd),
  Age =  mean(age)
  )
```

## Delta Method for a singly univariate parameter

First, we are going to derive the SE for the single probability or risk of death (the univariate case). We compute the risk of death in our sample and its variance as follows:  

```{r}
# Risk of death
p_death = mean(data$Y)
print(p_death)
```

```{r}
# Varianze for the risk of death
 n = nrow(data)
 var_p_death = p_death * (1 - p_death) / n
 print(var_p_death)
```

Now, let be f(x) = p. Then the first derivative of f′(x) = 1. So the variance of the risk of death can be estimated using (4) as: 

$$\text{Var(P(death))}\,=\,1\times\left[\frac{\text{p(1 - p)}}{n}\right].$$
 
This can be implemented in the following R code:
```{r}
dev_p_death = 1
se_risk = sqrt((dev_p_death) * var_p_death)
print(se_risk)
```

To check that our results are consistent with the implementation of the Delta Method function provide by the **msm** R package used for advanced Geographical Analysis [@kavroudakis2015].

```{r}
# install.packages("msm")
library(msm)
se_risk_delta = deltamethod(g = ~ x1, 
                            mean = p_death,         
                            cov  = var_p_death)
print(se_risk_delta)

cat("Are the same se_risk and se_risk_delta?")

ifelse(
  se_risk == se_risk_delta,
  print("Yes")
)
```

## Conditional Odds Ratio (COR): Multivariable Case

Let$'$s now compute the SE for the COR derived from a multivariable logistic regression model. Note that the COR transformation is a function of the regression coefficients from the logistic model. First we estimate the conditional probability of the risk of death for those patients with comorbidities adjusting for age. The model summary is described here below. The probability of death for a cancer patient in our sample with comorbidities compared with a patient without comorbidities and in average with the same age is approximately 40\% higher: 

```{r}
m1 <- glm(Y ~ age + cmbd, data = data, family = binomial)
summary(m1)
b1 <- coef(m1)[3]
cat("One-year mortality risk for patients with comorbidities vs no comorbidities is:") 
exp(b1)
```

We now, can derive the SE for the conditional OR using the formula (4) for the multivariate case. Note that the first derivative for the exponential function is equal to exponential and that to get the covariance of the parameter fitted in the model we use the command "vcov" in R.

```{r}
grad <- exp(b1)
vcov(m1)
vb1 <- vcov(m1)[3,3]
se <- grad %*% vb1 %*% grad
se_cor <- sqrt(se); se_cor
```

Now, we have to check that our results are consistent with the implementation of the Delta Method function provide by the **msm** R package  [@kavroudakis2015]
```{r}
se_cor_delta <- deltamethod(~ exp(x1), b1, vb1); se_cor_delta

cat("Are the same se_cor and se_cor_delta?")

ifelse(
  se_cor == se_cor_delta,
  print("Yes"),print("No")
)
```

## Conditional Risk Ratio (CRR): multivariable case

Let$'$s now compute the SE for the marginal multivariable RR derived from the predicted probabilities of a  multivariable logistic regression model. First we fit the model with the binary indicator of one-year mortality as dependent variable and patients$’$ age and comorbidities as independent variables. Then, from the fitted model and using the **predict** function we derive the probability of death from pancreatic cancer among patients aged 45 years and with comorbidities versus patients aged 75 years old with no comorbidities. Finally, we compute the conditional RR as the ratio between both probabilities. As we can see the risk of death in a population where cancer patients were aged 45 years with comorbidities is approximately 81\% higher than the risk of one-year mortality among cancer patients aged 75 years and with no comorbidities:

```{r}
m2 <- glm(Y ~ age + cmbd, data = data, family = binomial)
p75 <- predict(m2, newdata = data.frame(age = 75, cmbd = 0), type="response")
p45 <- predict(m2, newdata = data.frame(age = 45, cmbd = 1), type="response")
mrr <- p45 / p75;
cat("Conditional Risk Ratio: ", mrr)
```

Let$'$s now to compute the SE for the marginal RR. Note that the relative risk transformation is a function of the regression coefficients. First, we should define the conditional probability in terms of the regression coefficients. In our model, given patients age and comorbidities, the probability of one-year mortality is:
$$\text{P(Y=1|X)}\,=\,\frac{1}{1\,+\,\text{exp}(−\sum_{i=1}^k \beta(X))}$$
Where k is the number of parameters in the model $\beta\,=\,(\beta_{0},\beta_{1},\beta_{2})$. Therefore,
the probabality of one-year mortality for cancer patients aged 45 years with comorbidities is 
$$\text{P(1)(Y = 1|X1 = 45, X3 = 1)}\,=\,\frac{1}{1\,+\,exp(−\beta_{0}\,-\,\beta_{1}\times 45\,-\,\beta_{2}\times 1)},$$
and the probabality of one-year mortality for cancer patients aged 75 years with no comorbidities is 
$$\text{P(2)(Y = 1|X2 = 75, X4 = 0)}\,=\,\frac{1}{1\,+\,exp(−\beta_{0}\,-\,\beta_{1}\times 75\,-\,\beta_{2}\times 0)}.$$
Note that the CRR is a function of the regression coefficients from the logistic model. Thus, now we use the equation in (3) to get: 
$$\text{f(x)}\,=\,\frac{\frac{1}{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x1\,-\,\beta_{2} x3)}}{\frac{1}{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2}x4)}},$$
which simplifies to:
$$\text{f(x)}\,=\,\frac{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4)}{1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x1\,-\,\beta_{2}x3)}.$$
We now need to solve the partial derivatives for f(x) but one can use the online open source available software "Wolfram alpha: https://www.wolframalpha.com/" to readily get the results for the partial derivatives of f(x) and then apply the formula (3). Using the product and chain rules, we obtain the following partial derivatives:
$$\frac{df}{d\beta_{0}}\,=\,\text{−exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times \text{p}\,+\,(1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times \text{p}(1\,–\,\text{p}),$$
then,
$$\frac{df}{d\beta_{1}}\,=\,\text{−exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times x2 \times \text{p}\,+\,(1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times x1 \times \text{p}(1\,–\,\text{p}),$$
and, 
$$\frac{df}{d\beta_{2}}\,=\,\text{−exp}(−\beta_{0}\,-\,\beta_{1}x2\,-\,\beta_{2} x4))\times x4 \times \text{p}\,+\,(1\,+\,\text{exp}(−\beta_{0}\,-\,\beta_{1}x3\,-\,\beta_{2} x4))\times x3 \times \text{p}(1\,–\,\text{p})$$
where p is 
$$\text{P(Y=1|X1,X2)}\,=\,\frac{1}{1\,+\text{exp}(−\beta_{0}\,-\,\beta_{1}x1\,-\,\beta_{2}x3)},$$
i.e., the probability of one-year mortality for cancer patients aged 45 years with comorbidities. 

Let’s calculate our partial derivative in R as follows:

```{r}
x1 <- 45
x2 <- 75
x3 <- 1
x4 <- 0
b0 <- coef(m2)[1]
b1 <- coef(m2)[2]
b2 <- coef(m2)[3]
e1 <- exp(-b0 - 45*b1 - 1*b2)
e2 <- exp(-b0 - 75*b1 - 0*b2)
p1 <- 1 / (1 + e1)
p2 <- 1 / (1 + e2)
dfdb0 <- -e2*p1 + (1 + e2)*p1*(1 - p1)
dfdb1 <- -x2*e2*p1 + (1 + e2)*x1*p1*(1 - p1)
dfdb2 <- -x4*e2*p1 + (1 + e2)*x3*p1*(1 - p1)
grad <- c(dfdb0, dfdb1, dfdb2)
vG <- t(grad) %*% vcov(m2) %*% (grad)
se_crr <- c(sqrt(vG));se_crr
```

Now, let$'$s again to check if our results are consistent with the implementation of the Delta Method function provide by the **msm** R package [@kavroudakis2015]. We obtain the same results for the SE of the crr computed before (0.3653979)

```{r}
se_crr_delta <- deltamethod( ~(1 + exp(-x1 -75*x2 -0*x3)) / (1 + exp(-x1 -45*x2 -1*x3)), 
             c(b0, b1, b2), 
             vcov(m2)
             ); se_crr_delta
```
Finally, let$'$s compute the 95\% confidence intervals (CI):
```{r}
lb <- mrr - 1.96*sqrt(vG)
ub <- mrr + 1.96*sqrt(vG)
cat("\n Conditional Risk Rartio (95%CI): ") ; cat(mrr, "(", lb,",", ub,")")
```

## Marginal Causal Risk Ratio for the potential outcomes and based on the AIPTW estimator

Imagine now, that we want to emulate an impossible clinical trial where we would like to estimate the overall marginal one-year risk of death, standardized across all the levels of patients age. We are going to  contrast one-year mortality risk for a population where all patients have comorbidities versus another population where patients do not have comorbidities. When estimating the marginal causal risk ratio (MCRR) for a binary treatment (or exposure), methods that incorporate propensity scores, the G-computation, or a combination of both, namely double-robust methods, are preferred over naïve regression approaches which are biased under misspecification of a parametric outcome model. The Augmented Inverse Probability of Treatment Weighting (**AIPTW**) estimation is a double-robust two-step procedure with two equations (propensity score and mean outcome equations). Double-robust methods stem from based on semi-parametric theory (i.e., estimation equations) and only require the correct specification of one model. Here below we provide the formula to compute the MCRR: 

$$\text{Marginal Causal RR}^{AIPTW}\,=\,\frac{EY1}{EY0}\,=\,\text{log(EY1)}\,-\,\text{log(EY0)}.$$
Where 
$$\text{EY(1)}\,=\,\frac{1}{n}\sum_{i=1}^{n}\left(\frac{I\left(A_{i}=1\right)}{g_n(1|W_{i})}\right)\left[Y_{i}-\bar{Q}_{n}\left(A_{i},W_{i}\right)\right]+\frac{1}{n}\sum_{i=1}^{n}\bar{Q}_{n}\left(1,\ W_{i}\right),$$
and
$$\text{EY(0)}\,=\,\frac{1}{n}\sum_{i=1}^{n}\left(\frac{I\left(A_{i}=1\right)}{g_n(0|W_{i})}\right)\left[Y_{i}-\bar{Q}_{n}\left(A_{i},W_{i}\right)\right]+\frac{1}{n}\sum_{i=1}^{n}\bar{Q}_{n}\left(0,\ W_{i}\right).$$
Let$'$s use R to estimate the MCRR. Note that the MCRR is a function of other two functions, the marginal predictions of the regression coefficients from the propensity score logistic model and the marginal predictions from the outcome logistic regression model in case of a binary treatment and outcome. 

```{r}
# Step 1 estimation and prediction of the model for the outcome (G-computation)
gm <- glm(Y ~ age + cmbd, data = data, family = binomial(link=logit))
# Prediction for E(Y|A,W), and E(Y|cmbd = 1, age) and, E(Y|cmbd = 0, age)
QAW <- predict(gm, type = "response")
Q1W <- predict(gm, newdata = data.frame(cmbd = 1, age = data[ ,c("age")]), type = "response")
Q0W <- predict(gm, newdata = data.frame(cmbd = 0, age = data[ ,c("age")]), type = "response")
# Step 2 estimation and prediction of the propensity score (ps): E(A|W) or E(cmbd|age)
psm <- glm(cmbd ~ age*age*age, family = binomial, data = data)
gW = predict(psm, type = "response")
gbounds <- function(x,bounds=c(0.01,1)){
  x[x<min(bounds)] <- min(bounds)
  x[x>max(bounds)] <- max(bounds)
  return(x)
}
#gW <- gbounds(gW,bounds=c(0.05,0.975))
# MCRR
EY1 <- mean((data$Y) * (data$Y - Q1W) / gW + Q1W)
EY0 <- mean((1 - data$Y) * (data$Y - Q0W) / (1 - gW) + Q0W)
logMCRR <- log(EY1) - log(EY0) 
MCRR <- exp(logMCRR);MCRR
```
The marginal contrast of a population of patients where all of them have comorbidities versus other where patients do not have comorbidities after standardizing by the levels of patients age for the one-year mortality risk is 21 times higher for patients with comorbidities. However, we would like to get statistical inference for the estimate of the MCRR. Often researchers use the bootstrap procedure to derive 95\% confidence intervals (CI) but computing efficiency is low. However, we can use the functional Delta Method to apply the Central Limit Theorem and derive Wald type 95\% CI for the MCRR. Here we demonstrate how to derive the functional Delta Method based on the estimation of the efficient influence curve for the AIPTW estimator based on the formula (3) [@van2011]. In summary, for observed data $\text{O}_i$, i = 1, ..., n an asymptotically linear estimator $\hat\Psi$ of an estimand $\Psi$, is an estimator that can be represented as follows [@van2011]: 
$$\hat\Psi\,-\,\Psi\;=\;\frac{1}{n}\sum_{i=1}^n \text{D}(\text{O}_{i})\,+\,\text{Op}(\frac{1}{\sqrt(n)})\;\;(4).$$
Where  
$$\text{D}(\text{O}_{i})=f'(\theta)(\hat\theta\,-\,\theta).$$ 

Therefore for the log(MCRR) = log(EY1)-log(EY0), we have the following two partial derivatives:
$$\frac{\partial log(EY1,EY0)}{\partial EY1}\,=\, \frac{1}{EY1}, $$
and,
$$\frac{\partial log(EY0,EY1)}{\partial EY0}\,=\, \frac{1}{EY0}. $$
Let$'$s now call D1 and D0 the two functionals for the potential otucomes EY1 and EY0. Thus, based on $(\hat\theta\,-\,\theta)$ we get [@van2011]:
D1 = $$\frac{1}{n}\sum_{i=1}^{n}\left(\frac{I\left(A_{i}=1\right)}{g_n(1|W_{i})}\right)\left[Y_{i}-\bar{Q}_{n}\left(A_{i},W_{i}\right)\right]+\frac{1}{n}\sum_{i=1}^{n}\bar{Q}_{n}\left(1,\ W_{i}\right)\,-\,\text{E}(Y(1))$$
and,
D0 = $$\frac{1}{n}\sum_{i=1}^{n}\left(\frac{I\left(A_{i}=0\right)}{g_n(0|W_{i})}\right)\left[Y_{i}-\bar{Q}_{n}\left(A_{i},W_{i}\right)\right]+\frac{1}{n}\sum_{i=1}^{n}\bar{Q}_{n}\left(0,W_{i}\right)\,-\, \text{E}(Y(0)).$$
Where
$g_{n}$ is the propensity score for the treatment mechanism (A) and $\bar Q_{n}$ is the estimate of the conditional mean of outcome given treatment and confounders (W); $E(Y|A,W)$. Therefore
D($\text{O}_{i}$) for the log(MCRR)$\,=\,\frac{1}{EY1}D1\,+\,\frac{1}{EY0)}D0.$

```{r}
# Functional Delta Method for statisical inference
D1 <- (data$cmbd) * (data$Y - Q1W) / gW + Q1W - EY1 
D0 <- (1 - data$cmbd) * (data$Y - Q0W) / (1 - gW) + Q0W - EY0
# AIPTW CRR 95%CI
EIC <- ((1 / EY1) * D1) + ((1 / EY0) * D0)
varHat <- var(EIC) / n
CI <- c(exp(logMCRR - 1.96 * sqrt(varHat)), exp(logMCRR + 1.96 * sqrt(varHat))); MCRR; CI
```

Note: In general, when we have near positivity violations i.e., the positivity assumption states that within strata of W (age in our example) every patient had a nonzero probability of receiving either of the two treatment conditions (i.e. 0 <P(A=1|W)<1) the Bootstrap is preferred. 

# The Bootstrap
Finally, we are going to use a more conservative approach for statistical inference (i.e., the bootstrap). Using the Bootstrap the original sample approximates the population from which it was drawn. So resamples from this sample approximate what we would get if we took many samples from the population. The bootstrap distribution of a statistic, based on many resamples, approximates the sampling distribution of the statistic, based on many samples [@efron1982;@efron1983]. For statistical inference, we use the bootstrap standard error of a statistic is the standard deviation of the bootstrap distribution of that statistic. For most statistics, bootstrap distributions approximate the spread, bias, and shape of the
actual sampling distribution. The interval between 2.5 and 97.5 percentiles of the bootstrap distribution of a statistic is a 95\% bootstrap percentile confidence interval for the corresponding parameter [@efron1982;@efron1983].

Now let$'$s use the **boot** R package which provides extensive facilities for bootstrapping and related resampling methods. You can bootstrap a single statistic (e.g. a median), or a vector (e.g., regression weights). We are going to use the nonparametric bootstrapping [@boostrap2003].

```{r, warning=FALSE}
# Can get original estimate, by plugging in indices 1:n
library(boot)
aiptw.w = function(data,indices)
{
    dat=data[indices,]
    gm <- glm(Y ~ age + cmbd, data = dat, family = binomial(link=logit))
    # Prediction for E(Y|A,W), and E(Y|cmbd = 1, age) and, E(Y|cmbd = 0, age)
    Q1W <- predict(gm, newdata = data.frame(cmbd = 1, age = dat[ ,c("age")]), type = "response")
    Q0W <- predict(gm, newdata = data.frame(cmbd = 0, age = dat[ ,c("age")]), type = "response")
    # Step 2 estimation and prediction of the propensity score (ps): E(A|W) or E(cmbd|age)
    psm <- glm(cmbd ~ age, family = binomial, data = dat)
    gW = predict(psm, type = "response")
    # MCRR
    EY1 <- mean((data$Y) * (data$Y - Q1W) / gW + Q1W)
    EY0 <- mean((1 - data$Y) * (data$Y - Q0W) / (1 - gW) + Q0W)
    MCRR <- EY1 / EY0 
}
# Can get original estimate, by plugging in indices 1:n
aiptw.w(data,indices=1:nrow(data))
# Draw 200 bootstrap sample estimates 
boot.out=boot(data,aiptw.w,1000)
# compute confidence intervals using percentile method
boot.ci(boot.out,type="perc",conf=0.95)
```
Note that we got slightly narrower confidence intervals using the delta method compared to the Bootstrap procedure. It is because with the functional delta method in finite samples for causal inference the coverage decreases when there are violations of the positivity violations and it provide more precisse confidence intervals than other approaches such as the Bootastrap [@Dorie_2019]. In our empirical example the bootstrap compared to the delta method provide more conservative confidence intervals. 

# Conclusion
1. The Delta Method is widely used in classical epidemiological methods to derive the standard error (i.e., statistical inference) of functions of the coefficients of the parameters fitted in regression models. In casual inference is largely used because it eases the derivation of Wald type confidence intervals for data-adaptive double-robust estimators.
2. Recently, the study of the behaviour of the delta method for data-adaptive double-robust estimators in finite samples where there are positivity violations has shown that the functional delta method provides less conservative confidence intervals thant the Bootstrap. Simulations have shown that in such situations the coverage is poor [@Dorie_2019]. Therefore, more conservative and robust approaches such as the bootstrap procedure is still a valid and preferred method for statistical inference under violations and near violations of the positivity assumption. 
3. Users wanted to implemeted the bootstrap proceure using Targeted Maximum Likelihood Estimation would like to use the bootstrap TMLE package: https://github.com/wilsoncai1992/TMLEbootstrap

# Session Info 
```{r session-info}
devtools::session_info()
```

# Thank you  
Thank you for participating in this tutorial.  
If you have updates or changes that you would like to make, please send <a href="https://github.com/migariane/DeltaMethodEpi" target="_blank">me</a> a pull request.
Alternatively, if you have any questions, please e-mail me. 
You can cite this repository as:        
Luque-Fernandez MA, (2019). Delta Method in Epidemiology: An Applied and Reproducible Tutorial. GitHub repository, http://migariane.github.io/DeltaMethodEpi.nb.html.    
**Miguel Angel Luque Fernandez**     
**E-mail:** *miguel-angel.luque at lshtm.ac.uk*  
**Twitter** `@WATZILEI`  

# References 

